{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py    \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import nbimporter\n",
    "import gc\n",
    "\n",
    "from keras import backend as K \n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from scipy import stats\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore tensorflow deprecation warnings\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare GPU environment and define amount of memory to use\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"  # specify which GPU(s) to be used\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.42)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to load input data\n",
    "path = '/home/pablo/Documentos/lbl/olympics/final_runs_safety_data/input_features/'\n",
    "\n",
    "# Load input features from file\n",
    "features_filename = path + \"all_input_features.hdf5\"\n",
    "h5_results = h5py.File(features_filename, 'r')\n",
    "\n",
    "# Store features info\n",
    "signal = h5_results['signal'][:]\n",
    "h5_results.close()\n",
    "\n",
    "# Load input features from file\n",
    "features_filename = path + \"signal_features_m300.hdf5\"\n",
    "h5_results = h5py.File(features_filename, 'r')\n",
    "\n",
    "# Store features info\n",
    "signal2 = h5_results['signal'][:]\n",
    "h5_results.close()\n",
    "\n",
    "# Load extra background\n",
    "filename = path + \"all_input_features_extra_QCD.hdf5\"\n",
    "h5_results = h5py.File(filename, 'r')\n",
    "\n",
    "# Store reconstruction info\n",
    "background2 = h5_results['background'][:]\n",
    "\n",
    "h5_results.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Signal features shape: %s' % (signal.shape,))\n",
    "print('Background features shape: %s' % (background2.shape,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check memory used by dataframe\n",
    "print('Memory used by signal array: %.2f MB' % (signal.nbytes / (10**3 * 1024)))\n",
    "print('Memory used by background array: %.2f MB' % (background2.nbytes / (10**3 * 1024)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Signal sample event:\\n %s' % signal[-1])\n",
    "print('Background sample event:\\n %s' % background2[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study each feature of jet 1 before preprocessing data\n",
    "print('background2:')\n",
    "print('mj: (min, max) = (%f, %f)' % (background2[:,0].min(), background2[:,0].max()))\n",
    "print('t2: (min, max) = (%f, %f)' % (background2[:,1].min(), background2[:,1].max()))\n",
    "print('tau21: (min, max) = (%f, %f)' % (background2[:,2].min(), background2[:,2].max()))\n",
    "print('tau32: (min, max) = (%f, %f)' % (background2[:,3].min(), background2[:,3].max()))\n",
    "print('tau43: (min, max) = (%f, %f)' % (background2[:,4].min(), background2[:,4].max()))\n",
    "print('ntrk: (min, max) = (%i, %i)' % (background2[:,5].min(), background2[:,5].max()))\n",
    "print('pt: (min, max) = (%i, %i)' % (background2[:,6].min(), background2[:,6].max()))\n",
    "print('eta: (min, max) = (%i, %i)' % (background2[:,7].min(), background2[:,7].max()))\n",
    "print('phi: (min, max) = (%i, %i)' % (background2[:,8].min(), background2[:,8].max()))\n",
    "\n",
    "# Study each feature of jet 1 before preprocessing data\n",
    "print('SIGNAL:')\n",
    "print('mj: (min, max) = (%f, %f)' % (signal[:,0].min(), signal[:,0].max()))\n",
    "print('t2: (min, max) = (%f, %f)' % (signal[:,1].min(), signal[:,1].max()))\n",
    "print('tau21: (min, max) = (%f, %f)' % (signal[:,2].min(), signal[:,2].max()))\n",
    "print('tau32: (min, max) = (%f, %f)' % (signal[:,3].min(), signal[:,3].max()))\n",
    "print('tau43: (min, max) = (%f, %f)' % (signal[:,4].min(), signal[:,4].max()))\n",
    "print('ntrk: (min, max) = (%i, %i)' % (signal[:,5].min(), signal[:,5].max()))\n",
    "print('pt: (min, max) = (%i, %i)' % (signal[:,6].min(), signal[:,6].max()))\n",
    "print('eta: (min, max) = (%i, %i)' % (signal[:,7].min(), signal[:,7].max()))\n",
    "print('phi: (min, max) = (%i, %i)' % (signal[:,8].min(), signal[:,8].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study each feature of jet 2 before preprocessing data\n",
    "print('background2:')\n",
    "print('mj: (min, max) = (%f, %f)' % (background2[:,9].min(), background2[:,9].max()))\n",
    "print('t2: (min, max) = (%f, %f)' % (background2[:,10].min(), background2[:,10].max()))\n",
    "print('tau21: (min, max) = (%f, %f)' % (background2[:,11].min(), background2[:,11].max()))\n",
    "print('tau32: (min, max) = (%f, %f)' % (background2[:,12].min(), background2[:,12].max()))\n",
    "print('tau43: (min, max) = (%f, %f)' % (background2[:,13].min(), background2[:,13].max()))\n",
    "print('ntrk: (min, max) = (%i, %i)' % (background2[:,14].min(), background2[:,14].max()))\n",
    "print('pt: (min, max) = (%i, %i)' % (background2[:,15].min(), background2[:,15].max()))\n",
    "print('eta: (min, max) = (%i, %i)' % (background2[:,16].min(), background2[:,16].max()))\n",
    "print('phi: (min, max) = (%i, %i)' % (background2[:,17].min(), background2[:,17].max()))\n",
    "\n",
    "# Study each feature of jet 2 before preprocessing data\n",
    "print('SIGNAL:')\n",
    "print('mj: (min, max) = (%f, %f)' % (signal[:,9].min(), signal[:,9].max()))\n",
    "print('t2: (min, max) = (%f, %f)' % (signal[:,10].min(), signal[:,10].max()))\n",
    "print('tau21: (min, max) = (%f, %f)' % (signal[:,11].min(), signal[:,11].max()))\n",
    "print('tau32: (min, max) = (%f, %f)' % (signal[:,12].min(), signal[:,12].max()))\n",
    "print('tau43: (min, max) = (%f, %f)' % (signal[:,13].min(), signal[:,13].max()))\n",
    "print('ntrk: (min, max) = (%i, %i)' % (signal[:,14].min(), signal[:,14].max()))\n",
    "print('pt: (min, max) = (%i, %i)' % (signal[:,15].min(), signal[:,15].max()))\n",
    "print('eta: (min, max) = (%i, %i)' % (signal[:,16].min(), signal[:,16].max()))\n",
    "print('phi: (min, max) = (%i, %i)' % (signal[:,17].min(), signal[:,17].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to take the input features of the jets\n",
    "\n",
    "def make_data(jet1_features,jet2_features):\n",
    "    bgdata_jet1=background2[:,jet1_features]\n",
    "    bgdata_jet2=background2[:,jet2_features]\n",
    "\n",
    "    sigdata_jet1=signal[:,jet1_features]\n",
    "    sigdata_jet2=signal[:,jet2_features]\n",
    "\n",
    "    sigdata2_jet1=signal2[:,jet1_features]\n",
    "    sigdata2_jet2=signal2[:,jet2_features]\n",
    "\n",
    "    bg_mjj = background2[:,18]\n",
    "    sig_mjj = signal[:,18]\n",
    "    sig2_mjj=signal2[:,18]\n",
    "\n",
    "    bgdata_jets=np.hstack((bgdata_jet1,bgdata_jet2))\n",
    "    sigdata_jets=np.hstack((sigdata_jet1,sigdata_jet2))\n",
    "    sigdata2_jets=np.hstack((sigdata2_jet1,sigdata2_jet2))\n",
    "\n",
    "    # restrict mjj to 2800-5200 GeV \n",
    "#    bg_mask=(bg_mjj>2800) & (bg_mjj<5200)\n",
    "#    sig_mask=(sig_mjj>2800) & (sig_mjj<5200)\n",
    "#    sig2_mask=(sig2_mjj>2800) & (sig2_mjj<5200)\n",
    "#    \n",
    "#    bgdata_jets=np.hstack((bgdata_jet1,bgdata_jet2))[bg_mask]\n",
    "#    sigdata_jets=np.hstack((sigdata_jet1,sigdata_jet2))[sig_mask]\n",
    "#    sigdata2_jets=np.hstack((sigdata2_jet1,sigdata2_jet2))[sig2_mask]\n",
    "#    bg_mjj=bg_mjj[bg_mask]\n",
    "#    sig_mjj=sig_mjj[sig_mask]\n",
    "#    sig2_mjj=sig2_mjj[sig2_mask]\n",
    "    \n",
    "    meanvals=np.mean(bgdata_jets,axis=0)\n",
    "    stdvals=np.std(bgdata_jets,axis=0)\n",
    "\n",
    "    bgdata_std=(bgdata_jets-meanvals)/stdvals\n",
    "    sigdata_std=(sigdata_jets-meanvals)/stdvals\n",
    "    sigdata2_std=(sigdata2_jets-meanvals)/stdvals\n",
    "\n",
    "    #stdvec=np.array([400,1,1,400,400,400,1,1,400,400]).astype('float32')\n",
    "    #stdvec=stdvals\n",
    "    #stdvec=400*np.ones(bgdata_jets.shape[1]).astype('float32')\n",
    "    #bgdata_std=(bgdata_jets)/stdvec\n",
    "    #sigdata_std=(sigdata_jets)/stdvec\n",
    "    #sigdata2_std=(sigdata2_jets)/stdvec\n",
    "\n",
    "    bgdata_final=np.hstack((bgdata_std,bg_mjj.reshape((-1,1))))\n",
    "    sigdata_final=np.hstack((sigdata_std,sig_mjj.reshape((-1,1))))\n",
    "    sigdata2_final=np.hstack((sigdata2_std,sig2_mjj.reshape((-1,1))))\n",
    "    \n",
    "    return bgdata_final,sigdata_final,sigdata2_final, meanvals, stdvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the input features\n",
    "bgdata={}\n",
    "sigdata={}\n",
    "sigdata2={}\n",
    "\n",
    "# Define input features\n",
    "jet1_features=[0,2,3,5,6] # mj,tau21,tau32,ntrk,pt\n",
    "jet2_features=[9,11,12,14,15]\n",
    "\n",
    "# Put input features in a dictionary, whose key is defined as the number of features of each jet\n",
    "bgdata[5],sigdata[5],sigdata2[5], bg_mean, bg_std =make_data(jet1_features,jet2_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take data in the SR only:collection of True and False arrays of shape (n_bg, n_feat + mJJ), (n_sg, n_feat + mJJ)\n",
    "SRmask_bg=(bgdata[5][:,-1]>3500-200) & (bgdata[5][:,-1]<3500+200)      \n",
    "SRmask_sig=(sigdata[5][:,-1]>3500-200) & (sigdata[5][:,-1]<3500+200)      \n",
    "SRmask_sig2=(sigdata2[5][:,-1]>3500-200) & (sigdata2[5][:,-1]<3500+200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models weights and calculate mse\n",
    "bgout_final={}\n",
    "sigout_final={}\n",
    "sig2out_final={}\n",
    "mse_bg_final={}\n",
    "mse_sig_final={}\n",
    "mse_sig2_final={}\n",
    "\n",
    "\n",
    "for itrain in range(50):\n",
    "    bgout_final[itrain]=np.load('saved_weights_4/AEmodel_feat_5_lat_2_hid_512_mod_'\\\n",
    "                              +str(itrain)+'_nsig_0_0.dict_bg.npy')\n",
    "    sigout_final[itrain]=np.load('saved_weights_4/AEmodel_feat_5_lat_2_hid_512_mod_'\\\n",
    "                              +str(itrain)+'_nsig_0_0.dict_sig.npy')\n",
    "    sig2out_final[itrain]=np.load('saved_weights_4/AEmodel_feat_5_lat_2_hid_512_mod_'\\\n",
    "                              +str(itrain)+'_nsig_0_0.dict_sig2.npy')\n",
    "\n",
    "    mse_bg_final[itrain]=np.mean((bgout_final[itrain]-bgdata[5][:,:-1])**2,axis=1)\n",
    "    mse_sig_final[itrain]=np.mean((sigout_final[itrain]-sigdata[5][:,:-1])**2,axis=1)\n",
    "    mse_sig2_final[itrain]=np.mean((sig2out_final[itrain]-sigdata2[5][:,:-1])**2,axis=1)\n",
    "\n",
    "    \n",
    "del(bgout_final, sigout_final, sig2out_final)\n",
    "\n",
    "# Define averaged mse distributions\n",
    "mse_bg_final_avg=np.mean(np.array([mse_bg_final[i] for i in range(50)]),axis=0) # this averages the MSEs\n",
    "mse_sig_final_avg=np.mean(np.array([mse_sig_final[i] for i in range(50)]),axis=0) \n",
    "mse_sig2_final_avg=np.mean(np.array([mse_sig2_final[i] for i in range(50)]),axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate fpr, tpr and thresholds in the SR\n",
    "fpr,tpr,thresholds=roc_curve(np.concatenate((np.zeros(len(mse_bg_final_avg[SRmask_bg])),\n",
    "                                             np.ones(len(mse_sig_final_avg[SRmask_sig])))),\n",
    "                             np.concatenate((mse_bg_final_avg[SRmask_bg],\n",
    "                                             mse_sig_final_avg[SRmask_sig]))\n",
    "                            )\n",
    "\n",
    "\n",
    "# Calculate AUC\n",
    "auc_m500 = roc_auc_score(np.concatenate((np.zeros(len(mse_bg_final_avg[SRmask_bg])),\n",
    "                                         np.ones(len(mse_sig_final_avg[SRmask_sig])))),\n",
    "                         np.concatenate((mse_bg_final_avg[SRmask_bg],\n",
    "                                         mse_sig_final_avg[SRmask_sig]))\n",
    "                        )\n",
    "\n",
    "# Select thresholds with a finite SIC (not nan or inf), take the one that maximizes the SIC surve\n",
    "maxsiccut_sig=thresholds[np.isfinite(tpr/np.sqrt(fpr))][np.argmax((tpr/np.sqrt(fpr))[np.isfinite(tpr/np.sqrt(fpr))])]\n",
    "\n",
    "# Print max of SIC curve and the corresponding tpr\n",
    "SIC_m500 = (tpr/np.sqrt(fpr))[np.isfinite(tpr/np.sqrt(fpr))]\n",
    "thresh_m500 = thresholds[np.isfinite(tpr/np.sqrt(fpr))]\n",
    "\n",
    "print(np.max((tpr/np.sqrt(fpr))[np.isfinite(tpr/np.sqrt(fpr))]))\n",
    "print(tpr[np.argmax((tpr/np.sqrt(fpr))[np.isfinite(tpr/np.sqrt(fpr))])])\n",
    "print('AUC = %.3f' % auc_m500)\n",
    "fpr500=fpr\n",
    "tpr500=tpr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate fpr, tpr and thresholds in the SR\n",
    "fpr,tpr,thresholds=roc_curve(np.concatenate((np.zeros(len(mse_bg_final_avg[SRmask_bg])),\n",
    "                                             np.ones(len(mse_sig2_final_avg[SRmask_sig2])))),\n",
    "                             np.concatenate((mse_bg_final_avg[SRmask_bg],\n",
    "                                             mse_sig2_final_avg[SRmask_sig2]))\n",
    "                            )\n",
    "\n",
    "# Calculate AUC\n",
    "auc_m300 = roc_auc_score(np.concatenate((np.zeros(len(mse_bg_final_avg[SRmask_bg])),\n",
    "                                         np.ones(len(mse_sig2_final_avg[SRmask_sig2])))),\n",
    "                         np.concatenate((mse_bg_final_avg[SRmask_bg],\n",
    "                                         mse_sig2_final_avg[SRmask_sig2]))\n",
    "                        )\n",
    "\n",
    "# Select thresholds with a finite SIC (not nan or inf), take the one that maximizes the SIC surve\n",
    "maxsiccut_sig2=thresholds[np.isfinite(tpr/np.sqrt(fpr))][np.argmax((tpr/np.sqrt(fpr))[np.isfinite(tpr/np.sqrt(fpr))])]\n",
    "\n",
    "# Print max of SIC curve and the corresponding tpr\n",
    "SIC_m300 = (tpr/np.sqrt(fpr))[np.isfinite(tpr/np.sqrt(fpr))]\n",
    "thresh_m300 = thresholds[np.isfinite(tpr/np.sqrt(fpr))]\n",
    "\n",
    "print(np.max((tpr/np.sqrt(fpr))[np.isfinite(tpr/np.sqrt(fpr))]))\n",
    "print(tpr[np.argmax((tpr/np.sqrt(fpr))[np.isfinite(tpr/np.sqrt(fpr))])])\n",
    "print('AUC = %.3f' % auc_m300)\n",
    "fpr300=fpr\n",
    "tpr300=tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessing information for test data\n",
    "pre_info = np.array([bg_mean, bg_std])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################\n",
    "##########################################   Save to file   ###########################################\n",
    "#######################################################################################################    \n",
    "\n",
    "#n_background = len(bgdata[5])\n",
    "#n_signal = len(sigdata[5])\n",
    "#n_features = 10 + 1\n",
    "\n",
    "#save_path = '/home/pablo/Documentos/lbl/olympics/anomaly_detection_analysis/AE_new_data/'\n",
    "\n",
    "# Create dataset and classes to store important information\n",
    "#file_m500 = h5py.File(save_path + 'test_info_m500.hdf5', 'w')\n",
    "\n",
    "# Save mse distribution and AUC\n",
    "#bg_data_tofile = file_m500.create_dataset('bg_data', (n_background, n_features), dtype='f8')\n",
    "#sg_data_tofile = file_m500.create_dataset('sg_data', (n_signal, n_features), dtype='f8')\n",
    "#mse_bg_tofile = file_m500.create_dataset('mse_bg', (n_background, ), dtype='f8')\n",
    "#mse_sg_tofile = file_m500.create_dataset('mse_sg', (n_signal, ), dtype='f8')\n",
    "#pre_info_tofile = file_m500.create_dataset('preprocessing', (2, n_features-1), dtype='f8')\n",
    "\n",
    "# Fill dataset\n",
    "#bg_data_tofile[:] = bgdata[5]\n",
    "#sg_data_tofile[:] = sigdata[5]\n",
    "#mse_bg_tofile[:] = mse_bg_final_avg\n",
    "#mse_sg_tofile[:] = mse_sig_final_avg\n",
    "#pre_info_tofile[:] = pre_info\n",
    "\n",
    "#file_m500.close()\n",
    "\n",
    "\n",
    "# Create dataset and classes to store important information\n",
    "#file_m300 = h5py.File(save_path + 'test_info_m300.hdf5', 'w')\n",
    "\n",
    "# Save mse distribution and AUC\n",
    "#bg_data_tofile = file_m300.create_dataset('bg_data', (n_background, n_features), dtype='f8')\n",
    "#sg_data_tofile = file_m300.create_dataset('sg_data', (n_signal, n_features), dtype='f8')\n",
    "#mse_bg_tofile = file_m300.create_dataset('mse_bg', (n_background, ), dtype='f8')\n",
    "#mse_sg_tofile = file_m300.create_dataset('mse_sg', (n_signal, ), dtype='f8')\n",
    "#pre_info_tofile = file_m300.create_dataset('preprocessing', (2, n_features-1), dtype='f8')\n",
    "\n",
    "# Fill dataset\n",
    "#bg_data_tofile[:] = bgdata[5]\n",
    "#sg_data_tofile[:] = sigdata2[5]\n",
    "#mse_bg_tofile[:] = mse_bg_final_avg\n",
    "#mse_sg_tofile[:] = mse_sig2_final_avg\n",
    "#pre_info_tofile[:] = pre_info\n",
    "\n",
    "#file_m300.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input features from file\n",
    "#h5_results = h5py.File(save_path + 'test_info_m500.hdf5', 'r')\n",
    "\n",
    "#bg_evs = h5_results['bg_data'][:]\n",
    "#sg_evs = h5_results['sg_data'][:]\n",
    "#bg_mse = h5_results['mse_bg'][:]\n",
    "#sg_mse = h5_results['mse_sg'][:]\n",
    "#pre = h5_results['preprocessing'][:]\n",
    "\n",
    "#h5_results.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that returns a significance level\n",
    "from fit_utils import perform_fit, perform_fit2\n",
    "from scipy.stats import poisson, norm, kstest, skewnorm\n",
    "\n",
    "\n",
    "def Zscorefunc(obs,pred,sigma):\n",
    "    pred2=pred-sigma**2\n",
    "    LLR=0.25*(-2*obs+pred2**2/sigma**2+2*sigma**2+\n",
    "              pred2*(4-np.sqrt(pred2**2+4*obs*sigma**2)/sigma**2)\n",
    "             -4*obs*np.log((pred2+np.sqrt(pred2**2+4*obs*sigma**2))/(2*obs))\n",
    "             )\n",
    "    return np.sqrt(2*LLR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mJJ\n",
    "mjj_bg=bgdata[5][:,-1]\n",
    "mjj_sig=sigdata[5][:,-1]\n",
    "mjj_sig2=sigdata2[5][:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################\n",
    "###################################################################################################################\n",
    "##################################################               ##################################################\n",
    "##################################################     Cut 1     ##################################################\n",
    "##################################################               ##################################################\n",
    "###################################################################################################################\n",
    "###################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit with full dataset, no cuts\n",
    "#mask_sr=[3,4]\n",
    "#def fit_func(y,p0,p1,p2,p3,p4,p5,p6,p7):\n",
    "#    return np.exp(p0+p1*y+p2*y**2+p3*y**3+p4*y**4+p5*y**5+p6*y**6+p7*y**7)\n",
    "#def fit_func(x,p0,p1,p2,p3,p4,p5,p6):\n",
    "#    return (p3+p4*x+p5*x**2+p6*x**3)*skewnorm.pdf((x-p0)/p1,p2)\\\n",
    "#def fit_func(x,p0,p1,p2,p3,p4):\n",
    "#    return p0*(1-x)**(p1+p4*(np.log(1-x)))/(x**(p2+p3*np.log(x)))\n",
    "\n",
    "def fit_func(x,p0,p1):\n",
    "    return p0+p1*x\n",
    "\n",
    "pvallist_cut_sig=[]\n",
    "pvallist_cut_sig2=[]\n",
    "info_m500_cut_1 = []\n",
    "info_m300_cut_1 = []\n",
    "SIC_info_m500_cut_1 = []\n",
    "SIC_info_m300_cut_1 = []\n",
    "\n",
    "nbins=20\n",
    "mask_sr=[7,8,9,10,11,12]\n",
    "mask_sb=[x for x in range(nbins) if x not in mask_sr] \n",
    "\n",
    "nback = len(mjj_bg)\n",
    "\n",
    "for nsig in range(10,895,15):\n",
    "    print(nsig)\n",
    "    \n",
    "    ##########  m = 500 GeV  ##########\n",
    "\n",
    "    # Before cut: define bins and count number of signal and background events in each bin\n",
    "    counts_bg_raw,bins,_=plt.hist(mjj_bg,range=[3000,4000],bins=nbins,alpha=0.3)\n",
    "    counts_sig_raw,bins,_=plt.hist(mjj_sig[:nsig],range=[3000,4000],bins=nbins,alpha=0.3)\n",
    "    # Before cut: count number of signal and background events in the SR\n",
    "    Sinit=np.sum(counts_sig_raw[mask_sr])\n",
    "    Binit=np.sum(counts_bg_raw[mask_sr])\n",
    "    \n",
    "    # Find the mse threshold above which only 1% of the SR events survive\n",
    "    n_cut = int((Sinit + Binit)*0.99)\n",
    "    mse_bg_SR = mse_bg_final_avg[(mjj_bg > 3350) & (mjj_bg < 3650)]\n",
    "    mse_sg_SR = mse_sig_final_avg[:nsig][(mjj_sig[:nsig] > 3350) & (mjj_sig[:nsig] < 3650)]\n",
    "    mse_sorted = np.sort(np.concatenate((mse_bg_SR, mse_sg_SR), axis=0))\n",
    "    cut_1_mse_m500 = mse_sorted[n_cut]\n",
    "    \n",
    "    # Calculate signal efficiency that corresponds to this threshold\n",
    "    #up_bound_m500 = np.abs(SIC_m500 - maxsiccut_sig).argmin()\n",
    "    #eff_m500 = tpr500[:up_bound_m500][np.abs(SIC_m500[:up_bound_m500] - cut_1_mse_m500).argmin()]\n",
    "    \n",
    "    eff_m500 = tpr500[np.abs(thresh_m500 - cut_1_mse_m500).argmin()]\n",
    "    SIC_1_percent_m500 = SIC_m500[np.abs(thresh_m500 - cut_1_mse_m500).argmin()]\n",
    "    \n",
    "    print('mse threshold: %.5f' % cut_1_mse_m500)\n",
    "    print(Sinit)\n",
    "    print(Binit)\n",
    "    \n",
    "    # After cut: define bins and count number of signal and background events in each bin\n",
    "    counts_bg,bins,_=plt.hist(mjj_bg[mse_bg_final_avg>cut_1_mse_m500],range=[3000,4000],bins=nbins,alpha=0.3)\n",
    "    counts_sig,bins,_=plt.hist(mjj_sig[:nsig][mse_sig_final_avg[:nsig]>cut_1_mse_m500],range=[3000,4000],bins=nbins,alpha=0.3)\n",
    "    # After cut: count number of signal and background events in the SR\n",
    "    Safter=np.sum(counts_sig[mask_sr])\n",
    "    Bafter=np.sum(counts_bg[mask_sr])\n",
    "\n",
    "    # After cut: define the total number of events (s+b) in each SR bin, and the statistical error in each SR bin\n",
    "    ydata=(counts_bg+counts_sig).astype('float64')\n",
    "    yerr=np.sqrt(counts_bg+counts_sig).astype('float64')\n",
    "    xdata=(0.5*(bins[:-1]+bins[1:])/14000).astype('float64')\n",
    "\n",
    "    plt.axvline(bins[mask_sr[0]])\n",
    "    plt.axvline(bins[mask_sr[-1]+1])\n",
    "\n",
    "    # Make the fit\n",
    "    popt,pcov,ydata_fit,pval = perform_fit(ydata,yerr,xdata,fit_func,mask_sb,mask_sr)\n",
    "    \n",
    "    # Check that the function is a good fit to the sideband\n",
    "    residuals = (ydata - ydata_fit)/yerr\n",
    "    redchisq=np.sum((residuals[mask_sb])**2/(len(mask_sb)-len(popt)))\n",
    "    print(\"GOODNESS OF FIT \",redchisq)\n",
    "    pvallist_cut_sig.append([nsig,Sinit/Binit,Sinit/np.sqrt(Binit),pval,norm.ppf(1-pval), Safter/np.sqrt(Bafter)])\n",
    "    \n",
    "    # Save useful information\n",
    "    S_B = Sinit/Binit\n",
    "    S_sqrt_B_initial = Sinit/np.sqrt(Binit)\n",
    "    S_sqrt_B_after = Safter/np.sqrt(Bafter)\n",
    "    pval_expected_initial = 1-norm.cdf(S_sqrt_B_initial)\n",
    "    pval_expected_after = 1-norm.cdf(S_sqrt_B_after)\n",
    "    selection_m500 = (Safter + Bafter)/(Sinit + Binit)*100\n",
    "    \n",
    "    info_m500_cut_1.append([S_B, S_sqrt_B_initial, S_sqrt_B_after, pval, pval_expected_initial, pval_expected_after, selection_m500])\n",
    "    SIC_info_m500_cut_1.append([S_B, eff_m500, SIC_1_percent_m500])\n",
    "    \n",
    "    \n",
    "    ##########  m = 300 GeV  ##########\n",
    "\n",
    "    # Before cut: define bins and count number of signal and background events in each bin\n",
    "    counts_bg_raw,bins,_=plt.hist(mjj_bg,range=[3000,4000],bins=nbins,alpha=0.3)\n",
    "    counts_sig2_raw,bins,_=plt.hist(mjj_sig2[:nsig],range=[3000,4000],bins=nbins,alpha=0.3)\n",
    "    # Before cut: count number of signal and background events in the SR\n",
    "    Sinit=np.sum(counts_sig2_raw[mask_sr])\n",
    "    Binit=np.sum(counts_bg_raw[mask_sr])\n",
    "    \n",
    "    # Find the mse threshold above which only 1% of the SR events survive\n",
    "    n_cut = int((Sinit + Binit)*0.99)\n",
    "    mse_bg_SR = mse_bg_final_avg[(mjj_bg > 3350) & (mjj_bg < 3650)]\n",
    "    mse_sg_SR = mse_sig2_final_avg[:nsig][(mjj_sig2[:nsig] > 3350) & (mjj_sig2[:nsig] < 3650)]\n",
    "    mse_sorted = np.sort(np.concatenate((mse_bg_SR, mse_sg_SR), axis=0))\n",
    "    cut_1_mse_m300 = mse_sorted[n_cut]\n",
    "    \n",
    "    # Calculate signal efficiency that corresponds to this threshold\n",
    "    #up_bound_m300 = np.abs(SIC_m300 - maxsiccut_sig).argmin()\n",
    "    #eff_m300 = tpr300[:up_bound_m300][np.abs(SIC_m300[:up_bound_m300] - cut_1_mse_m300).argmin()]\n",
    "    \n",
    "    eff_m300 = tpr300[np.abs(thresh_m300 - cut_1_mse_m300).argmin()]\n",
    "    SIC_1_percent_m300 = SIC_m300[np.abs(thresh_m300 - cut_1_mse_m300).argmin()]\n",
    "\n",
    "    # After cut: define bins and count number of signal and background events in each bin\n",
    "    counts_bg,bins,_=plt.hist(mjj_bg[mse_bg_final_avg>cut_1_mse_m300],range=[3000,4000],bins=nbins,alpha=0.3)\n",
    "    counts_sig2,bins,_=plt.hist(mjj_sig2[:nsig][mse_sig2_final_avg[:nsig]>cut_1_mse_m300],range=[3000,4000],bins=nbins,alpha=0.3)\n",
    "    # After cut: count number of signal and background events in the SR\n",
    "    Safter=np.sum(counts_sig2[mask_sr])\n",
    "    Bafter=np.sum(counts_bg[mask_sr])\n",
    "    \n",
    "    # After cut: define the total number of events (s+b) in each bin, and the statistical error in each bin\n",
    "    ydata=(counts_bg+counts_sig2).astype('float64')\n",
    "    yerr=np.sqrt(counts_bg+counts_sig2).astype('float64')\n",
    "    xdata=(0.5*(bins[:-1]+bins[1:])/14000).astype('float64')\n",
    "    plt.axvline(bins[mask_sr[0]])\n",
    "    plt.axvline(bins[mask_sr[-1]+1])\n",
    "\n",
    "    # Make the fit\n",
    "    popt,pcov,ydata_fit,pval = perform_fit(ydata,yerr,xdata,fit_func,mask_sb,mask_sr)\n",
    "    \n",
    "    #Check that the function is a good fit to the sideband\n",
    "    residuals = (ydata - ydata_fit)/yerr\n",
    "    redchisq=np.sum((residuals[mask_sb])**2/(len(mask_sb)-len(popt)))\n",
    "    print(\"GOODNESS OF FIT \",redchisq)\n",
    "    pvallist_cut_sig2.append([nsig,Sinit/Binit,Sinit/np.sqrt(Binit),pval,norm.ppf(1-pval), Safter/np.sqrt(Bafter)])\n",
    "    \n",
    "    # Save useful information\n",
    "    S_B = Sinit/Binit\n",
    "    S_sqrt_B_initial = Sinit/np.sqrt(Binit)\n",
    "    S_sqrt_B_after = Safter/np.sqrt(Bafter)\n",
    "    pval_expected_initial = 1-norm.cdf(S_sqrt_B_initial)\n",
    "    pval_expected_after = 1-norm.cdf(S_sqrt_B_after)\n",
    "    selection_m300 = (Safter + Bafter)/(Sinit + Binit)*100\n",
    "    \n",
    "    info_m300_cut_1.append([S_B, S_sqrt_B_initial, S_sqrt_B_after, pval, pval_expected_initial, pval_expected_after, selection_m300])\n",
    "    SIC_info_m300_cut_1.append([S_B, eff_m300, SIC_1_percent_m300])\n",
    "    \n",
    "# Transform lists to arrays\n",
    "info_m500_cut_1 = np.array(info_m500_cut_1)\n",
    "info_m300_cut_1 = np.array(info_m300_cut_1)\n",
    "SIC_info_m500_cut_1 = np.array(SIC_info_m500_cut_1)\n",
    "SIC_info_m300_cut_1 = np.array(SIC_info_m300_cut_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Event selection for m500: %.2f%%' % selection_m500)\n",
    "print('Event selection for m300: %.2f%%' % selection_m300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Signal efficiency for this cut on m500:\\n%s\\n' % SIC_info_m500_cut_1[:,1])\n",
    "print('Signal efficiency for this cut on m300:\\n%s' % SIC_info_m300_cut_1[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SIC value for this cut on m500:\\n%s\\n' % SIC_info_m500_cut_1[:,2])\n",
    "print('SIC value for this cut on m300:\\n%s' % SIC_info_m300_cut_1[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Threshold based on the SIC curve: mse = %.3f' % cut_1_mse_m500)\n",
    "print('Threshold based on the SIC curve: mse = %.3f' % cut_1_mse_m300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plot\n",
    "plt.plot(info_m500_cut_1[:,0], info_m500_cut_1[:,3], label='1% overall',color='orange')\n",
    "plt.plot(info_m500_cut_1[:,0], info_m500_cut_1[:,5], label='1%, S/sqrtB',color='orange',ls='dashed')\n",
    "\n",
    "plt.plot(info_m500_cut_1[:,0], info_m500_cut_1[:,3], label='optimal cut, fit',color='blue')\n",
    "plt.plot(info_m500_cut_1[:,0], info_m500_cut_1[:,5], label='optimal cut, S/sqrtB',color='blue',ls='dashed')\n",
    "plt.plot(info_m500_cut_1[:,0], info_m500_cut_1[:,4], label='no cut, S/sqrtB',color='red',ls='dashed')\n",
    "\n",
    "for sigma in range(1,9):\n",
    "    plt.axhline(1-norm.cdf(sigma),linestyle='dashed',color='black')\n",
    "    sigmastring = r'$' + str(sigma) + '\\sigma$'\n",
    "    plt.text(0.0015,(1-norm.cdf(sigma))*1.1,sigmastring,va='bottom',ha='center',fontsize=12)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.ylim(1e-15,1)\n",
    "plt.title('m=500 GeV')\n",
    "plt.legend()\n",
    "plt.xlabel('S/B in SR')\n",
    "plt.ylabel('p value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plot\n",
    "plt.plot(info_m300_cut_1[:,0], info_m300_cut_1[:,3], label='optimal cut, fit',color='blue')\n",
    "plt.plot(info_m300_cut_1[:,0], info_m300_cut_1[:,5], label='optimal cut, S/sqrtB',color='blue',ls='dashed')\n",
    "plt.plot(info_m300_cut_1[:,0], info_m300_cut_1[:,4], label='no cut, S/sqrtB',color='red',ls='dashed')\n",
    "\n",
    "for sigma in range(1,9):\n",
    "    plt.axhline(1-norm.cdf(sigma),linestyle='dashed',color='black')\n",
    "    sigmastring = r'$' + str(sigma) + '\\sigma$'\n",
    "    plt.text(0.0015,(1-norm.cdf(sigma))*1.1,sigmastring,va='bottom',ha='center',fontsize=12)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.ylim(1e-15,1)\n",
    "plt.title('m=300 GeV')\n",
    "plt.legend()\n",
    "plt.xlabel('S/B in SR')\n",
    "plt.ylabel('p value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the fraction of selected events in the SR for each benchmarks for the m500 signal\n",
    "info_m500_cut_1[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the fraction of selected events in the SR for each benchmarks for the m300 signal\n",
    "info_m300_cut_1[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################\n",
    "###################################################################################################################\n",
    "##################################################               ##################################################\n",
    "##################################################     Cut 2     ##################################################\n",
    "##################################################               ##################################################\n",
    "###################################################################################################################\n",
    "###################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit with full dataset, no cuts\n",
    "#mask_sr=[3,4]\n",
    "#def fit_func(y,p0,p1,p2,p3,p4,p5,p6,p7):\n",
    "#    return np.exp(p0+p1*y+p2*y**2+p3*y**3+p4*y**4+p5*y**5+p6*y**6+p7*y**7)\n",
    "#def fit_func(x,p0,p1,p2,p3,p4,p5,p6):\n",
    "#    return (p3+p4*x+p5*x**2+p6*x**3)*skewnorm.pdf((x-p0)/p1,p2)\\\n",
    "#def fit_func(x,p0,p1,p2,p3,p4):\n",
    "#    return p0*(1-x)**(p1+p4*(np.log(1-x)))/(x**(p2+p3*np.log(x)))\n",
    "\n",
    "def fit_func(x,p0,p1):\n",
    "    return p0+p1*x\n",
    "\n",
    "#mask_sr=[4,5]\n",
    "#mask_sr=[11,12,13,14,15,16]\n",
    "pvallist_cut_sig=[]\n",
    "pvallist_cut_sig2=[]\n",
    "info_m500_cut_2 = []\n",
    "info_m300_cut_2 = []\n",
    "SIC_info_m500_cut_2 = []\n",
    "SIC_info_m300_cut_2 = []\n",
    "\n",
    "nbins=32\n",
    "#mask_sr=[7,8,9,10,11,12]\n",
    "#mask_sr=[9,10,11,12,13,14]\n",
    "mask_sr=[11,12,13,14,15,16]\n",
    "mask_sb=[x for x in range(nbins) if x not in mask_sr] \n",
    "\n",
    "nback = len(mjj_bg)\n",
    "\n",
    "for nsig in range(10,895,15):\n",
    "    print(nsig)\n",
    "    \n",
    "    ##########  m = 500 GeV  ##########\n",
    "\n",
    "    # Before cut: define bins and count number of signal and background events in each bin\n",
    "    counts_bg_raw,bins,_=plt.hist(mjj_bg,range=[2800,4400],bins=nbins,alpha=0.3)\n",
    "    counts_sig_raw,bins,_=plt.hist(mjj_sig[:nsig],range=[2800,4400],bins=nbins,alpha=0.3)\n",
    "    # Before cut: count number of signal and background events in the SR\n",
    "    Sinit=np.sum(counts_sig_raw[mask_sr])\n",
    "    Binit=np.sum(counts_bg_raw[mask_sr])\n",
    "    \n",
    "    # Find the mse threshold above which only 0.1% of the SR events survive\n",
    "    n_cut = int((Sinit + Binit)*0.999)\n",
    "    mse_bg_SR = mse_bg_final_avg[(mjj_bg > 3350) & (mjj_bg < 3650)]\n",
    "    mse_sg_SR = mse_sig_final_avg[:nsig][(mjj_sig[:nsig] > 3350) & (mjj_sig[:nsig] < 3650)]\n",
    "    mse_sorted = np.sort(np.concatenate((mse_bg_SR, mse_sg_SR), axis=0))\n",
    "    cut_2_mse_m500 = mse_sorted[n_cut]\n",
    "    \n",
    "    # Calculate signal efficiency that corresponds to this threshold\n",
    "    #up_bound_m500 = np.abs(SIC_m500 - maxsiccut_sig).argmin()\n",
    "    #eff_m500 = tpr500[:up_bound_m500][np.abs(SIC_m500[:up_bound_m500] - cut_2_mse_m500).argmin()]\n",
    "    \n",
    "    eff_m500 = tpr500[np.abs(thresh_m500 - cut_2_mse_m500).argmin()]\n",
    "    SIC_01_percent_m500 = SIC_m500[np.abs(thresh_m500 - cut_2_mse_m500).argmin()]\n",
    "    \n",
    "    #print('mse threshold: %.5f' % cut_2_mse_m500)\n",
    "    \n",
    "    # After cut: define bins and count number of signal and background events in each bin\n",
    "    counts_bg,bins,_=plt.hist(mjj_bg[mse_bg_final_avg>cut_2_mse_m500],range=[2800,4400],bins=nbins,alpha=0.3)\n",
    "    counts_sig,bins,_=plt.hist(mjj_sig[:nsig][mse_sig_final_avg[:nsig]>cut_2_mse_m500],range=[2800,4400],bins=nbins,alpha=0.3)\n",
    "    # After cut: count number of signal and background events in the SR\n",
    "    Safter=np.sum(counts_sig[mask_sr])\n",
    "    Bafter=np.sum(counts_bg[mask_sr])\n",
    "\n",
    "    # After cut: define the total number of events (s+b) in each SR bin, and the statistical error in each SR bin\n",
    "    ydata=(counts_bg+counts_sig).astype('float64')\n",
    "    yerr=np.sqrt(counts_bg+counts_sig).astype('float64')\n",
    "    xdata=(0.5*(bins[:-1]+bins[1:])/14000).astype('float64')\n",
    "\n",
    "    plt.axvline(bins[mask_sr[0]])\n",
    "    plt.axvline(bins[mask_sr[-1]+1])\n",
    "\n",
    "    # Make the fit\n",
    "    popt,pcov,ydata_fit,pval = perform_fit(ydata,yerr,xdata,fit_func,mask_sb,mask_sr)\n",
    "    \n",
    "    # Check that the function is a good fit to the sideband\n",
    "    residuals = (ydata - ydata_fit)/yerr\n",
    "    redchisq=np.sum((residuals[mask_sb])**2/(len(mask_sb)-len(popt)))\n",
    "    print(\"GOODNESS OF FIT \",redchisq)\n",
    "    pvallist_cut_sig.append([nsig,Sinit/Binit,Sinit/np.sqrt(Binit),pval,norm.ppf(1-pval), Safter/np.sqrt(Bafter)])\n",
    "    \n",
    "    # Save useful information\n",
    "    S_B = Sinit/Binit\n",
    "    S_sqrt_B_initial = Sinit/np.sqrt(Binit)\n",
    "    S_sqrt_B_after = Safter/np.sqrt(Bafter)\n",
    "    pval_expected_initial = 1-norm.cdf(S_sqrt_B_initial)\n",
    "    pval_expected_after = 1-norm.cdf(S_sqrt_B_after)\n",
    "    selection_m500 = (Safter + Bafter)/(Sinit + Binit)*100\n",
    "    \n",
    "    print(cut_2_mse_m500)\n",
    "    \n",
    "    info_m500_cut_2.append([S_B, S_sqrt_B_initial, S_sqrt_B_after, pval, pval_expected_initial, pval_expected_after, selection_m500])\n",
    "    SIC_info_m500_cut_2.append([S_B, eff_m500, SIC_01_percent_m500])\n",
    "    \n",
    "    \n",
    "    ##########  m = 300 GeV  ##########\n",
    "\n",
    "    # Before cut: define bins and count number of signal and background events in each bin\n",
    "    counts_bg_raw,bins,_=plt.hist(mjj_bg,range=[2800,4400],bins=nbins,alpha=0.3)\n",
    "    counts_sig2_raw,bins,_=plt.hist(mjj_sig2[:nsig],range=[2800,4400],bins=nbins,alpha=0.3)\n",
    "    # Before cut: count number of signal and background events in the SR\n",
    "    Sinit=np.sum(counts_sig2_raw[mask_sr])\n",
    "    Binit=np.sum(counts_bg_raw[mask_sr])\n",
    "    \n",
    "    # Find the mse threshold above which only 0.1% of the SR events survive\n",
    "    n_cut = int((Sinit + Binit)*0.999)\n",
    "    mse_bg_SR = mse_bg_final_avg[(mjj_bg > 3350) & (mjj_bg < 3650)]\n",
    "    mse_sg_SR = mse_sig2_final_avg[:nsig][(mjj_sig2[:nsig] > 3350) & (mjj_sig2[:nsig] < 3650)]\n",
    "    mse_sorted = np.sort(np.concatenate((mse_bg_SR, mse_sg_SR), axis=0))\n",
    "    cut_2_mse_m300 = mse_sorted[n_cut]\n",
    "    \n",
    "    # Calculate signal efficiency that corresponds to this threshold\n",
    "    #up_bound_m300 = np.abs(SIC_m300 - maxsiccut_sig).argmin()\n",
    "    #eff_m300 = tpr300[:up_bound_m300][np.abs(SIC_m300[:up_bound_m300] - cut_2_mse_m300).argmin()]\n",
    "    \n",
    "    eff_m300 = tpr300[np.abs(thresh_m300 - cut_2_mse_m300).argmin()]\n",
    "    SIC_01_percent_m300 = SIC_m300[np.abs(thresh_m300 - cut_2_mse_m300).argmin()]\n",
    "\n",
    "    # After cut: define bins and count number of signal and background events in each bin\n",
    "    counts_bg,bins,_=plt.hist(mjj_bg[mse_bg_final_avg>cut_2_mse_m300],range=[2800,4400],bins=nbins,alpha=0.3)\n",
    "    counts_sig2,bins,_=plt.hist(mjj_sig2[:nsig][mse_sig2_final_avg[:nsig]>cut_2_mse_m300],range=[2800,4400],bins=nbins,alpha=0.3)\n",
    "    # After cut: count number of signal and background events in the SR\n",
    "    Safter=np.sum(counts_sig2[mask_sr])\n",
    "    Bafter=np.sum(counts_bg[mask_sr])\n",
    "    \n",
    "    # After cut: define the total number of events (s+b) in each bin, and the statistical error in each bin\n",
    "    ydata=(counts_bg+counts_sig2).astype('float64')\n",
    "    yerr=np.sqrt(counts_bg+counts_sig2).astype('float64')\n",
    "    xdata=(0.5*(bins[:-1]+bins[1:])/14000).astype('float64')\n",
    "    plt.axvline(bins[mask_sr[0]])\n",
    "    plt.axvline(bins[mask_sr[-1]+1])\n",
    "\n",
    "    # Make the fit\n",
    "    popt,pcov,ydata_fit,pval = perform_fit(ydata,yerr,xdata,fit_func,mask_sb,mask_sr)\n",
    "    \n",
    "    #Check that the function is a good fit to the sideband\n",
    "    residuals = (ydata - ydata_fit)/yerr\n",
    "    redchisq=np.sum((residuals[mask_sb])**2/(len(mask_sb)-len(popt)))\n",
    "    print(\"GOODNESS OF FIT \",redchisq)\n",
    "    pvallist_cut_sig2.append([nsig,Sinit/Binit,Sinit/np.sqrt(Binit),pval,norm.ppf(1-pval), Safter/np.sqrt(Bafter)])\n",
    "    \n",
    "    # Save useful information\n",
    "    S_B = Sinit/Binit\n",
    "    S_sqrt_B_initial = Sinit/np.sqrt(Binit)\n",
    "    S_sqrt_B_after = Safter/np.sqrt(Bafter)\n",
    "    pval_expected_initial = 1-norm.cdf(S_sqrt_B_initial)\n",
    "    pval_expected_after = 1-norm.cdf(S_sqrt_B_after)\n",
    "    selection_m300 = (Safter + Bafter)/(Sinit + Binit)*100\n",
    "    \n",
    "    info_m300_cut_2.append([S_B, S_sqrt_B_initial, S_sqrt_B_after, pval, pval_expected_initial, pval_expected_after, selection_m300])\n",
    "    SIC_info_m300_cut_2.append([S_B, eff_m300, SIC_01_percent_m300])\n",
    "    \n",
    "# Transform lists to arrays\n",
    "info_m500_cut_2 = np.array(info_m500_cut_2)\n",
    "info_m300_cut_2 = np.array(info_m300_cut_2)\n",
    "SIC_info_m500_cut_2 = np.array(SIC_info_m500_cut_2)\n",
    "SIC_info_m300_cut_2 = np.array(SIC_info_m300_cut_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Event selection for m500: %.2f%%' % selection_m500)\n",
    "print('Event selection for m300: %.2f%%' % selection_m300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Signal efficiency for this cut on m500:\\n%s\\n' % SIC_info_m500_cut_2[:,1])\n",
    "print('Signal efficiency for this cut on m300:\\n%s' % SIC_info_m300_cut_2[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SIC value for this cut on m500:\\n%s\\n' % SIC_info_m500_cut_2[:,2])\n",
    "print('SIC value for this cut on m300:\\n%s' % SIC_info_m300_cut_2[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Threshold based on the SIC curve: mse = %.3f' % cut_2_mse_m500)\n",
    "print('Threshold based on the SIC curve: mse = %.3f' % cut_2_mse_m300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plot\n",
    "plt.plot(info_m500_cut_4[:,0], info_m500_cut_4[:,3], label='0.1% small range',color='orange')\n",
    "plt.plot(info_m500_cut_4[:,0], info_m500_cut_4[:,5], label='0.1%, S/sqrtB',color='orange',ls='dashed')\n",
    "\n",
    "plt.plot(info_m500_cut_2[:,0], info_m500_cut_2[:,3], label='optimal cut, fit',color='blue')\n",
    "plt.plot(info_m500_cut_2[:,0], info_m500_cut_2[:,5], label='optimal cut, S/sqrtB',color='blue',ls='dashed')\n",
    "plt.plot(info_m500_cut_2[:,0], info_m500_cut_2[:,4], label='no cut, S/sqrtB',color='red',ls='dashed')\n",
    "\n",
    "for sigma in range(1,9):\n",
    "    plt.axhline(1-norm.cdf(sigma),linestyle='dashed',color='black')\n",
    "    sigmastring = r'$' + str(sigma) + '\\sigma$'\n",
    "    plt.text(0.0015,(1-norm.cdf(sigma))*1.1,sigmastring,va='bottom',ha='center',fontsize=12)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.ylim(1e-15,1)\n",
    "plt.title('m=500 GeV')\n",
    "plt.legend()\n",
    "plt.xlabel('S/B in SR')\n",
    "plt.ylabel('p value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plot\n",
    "plt.plot(info_m300_cut_2[:,0], info_m300_cut_2[:,3], label='optimal cut, fit',color='blue')\n",
    "plt.plot(info_m300_cut_2[:,0], info_m300_cut_2[:,5], label='optimal cut, S/sqrtB',color='blue',ls='dashed')\n",
    "plt.plot(info_m300_cut_2[:,0], info_m300_cut_2[:,4], label='no cut, S/sqrtB',color='red',ls='dashed')\n",
    "\n",
    "for sigma in range(1,9):\n",
    "    plt.axhline(1-norm.cdf(sigma),linestyle='dashed',color='black')\n",
    "    sigmastring = r'$' + str(sigma) + '\\sigma$'\n",
    "    plt.text(0.0015,(1-norm.cdf(sigma))*1.1,sigmastring,va='bottom',ha='center',fontsize=12)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.ylim(1e-15,1)\n",
    "plt.title('m=300 GeV')\n",
    "plt.legend()\n",
    "plt.xlabel('S/B in SR')\n",
    "plt.ylabel('p value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the fraction of selected events in the SR for each benchmarks for the m500 signal\n",
    "info_m500_cut_2[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the fraction of selected events in the SR for each benchmarks for the m300 signal\n",
    "info_m300_cut_2[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of scenarios:\n",
    "# 1) Cut 1: 1% cut on SR events, fit range [3000, 4000]         |  Best one   |  sigma = 0.56 for S/B = 0\n",
    "# 2) Cut 2: 0.1% cut on SR events, fit range [2800, 4400]       |  Best one   |  sigma = 1.06 for S/B = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################\n",
    "##########################################   Save to file   ###########################################\n",
    "#######################################################################################################    \n",
    "\n",
    "save_path = '/home/pablo/Documentos/lbl/olympics/anomaly_detection_analysis/AE_new_data/'\n",
    "\n",
    "# Create dataset and classes to store important information\n",
    "file_m500 = h5py.File(save_path + 'AE_cuts_fit_SIC_info_m500.hdf5', 'w')\n",
    "\n",
    "# Save mse distribution and AUC\n",
    "cut_1_percent_info_tofile = file_m500.create_dataset('cut_1_percent', SIC_info_m500_cut_3.shape, dtype='f8')\n",
    "cut_01_percent_info_tofile = file_m500.create_dataset('cut_01_percent', SIC_info_m500_cut_6.shape, dtype='f8')\n",
    "\n",
    "# Fill dataset\n",
    "cut_1_percent_info_tofile[:] = SIC_info_m500_cut_1\n",
    "cut_01_percent_info_tofile[:] = SIC_info_m500_cut_2\n",
    "\n",
    "file_m500.close()\n",
    "\n",
    "\n",
    "# Create dataset and classes to store important information\n",
    "file_m300 = h5py.File(save_path + 'AE_cuts_fit_SIC_info_m300.hdf5', 'w')\n",
    "\n",
    "# Save mse distribution and AUC\n",
    "cut_1_percent_info_tofile = file_m300.create_dataset('cut_1_percent', SIC_info_m300_cut_3.shape, dtype='f8')\n",
    "cut_01_percent_info_tofile = file_m300.create_dataset('cut_01_percent', SIC_info_m300_cut_6.shape, dtype='f8')\n",
    "\n",
    "# Fill dataset\n",
    "cut_1_percent_info_tofile[:] = SIC_info_m300_cut_1\n",
    "cut_01_percent_info_tofile[:] = SIC_info_m300_cut_2\n",
    "\n",
    "file_m300.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################\n",
    "##########################################   Save to file   ###########################################\n",
    "#######################################################################################################    \n",
    "\n",
    "save_path = '/home/pablo/Documentos/lbl/olympics/anomaly_detection_analysis/AE_new_data/'\n",
    "\n",
    "# Create dataset and classes to store important information\n",
    "file_m500 = h5py.File(save_path + 'AE_pvalues_from_fit_info_m500.hdf5', 'w')\n",
    "\n",
    "# Save mse distribution and AUC\n",
    "fit_cut_1_info_tofile = file_m500.create_dataset('fit_m500_cut_1', info_m500_cut_1.shape, dtype='f8')\n",
    "fit_cut_2_info_tofile = file_m500.create_dataset('fit_m500_cut_2', info_m500_cut_2.shape, dtype='f8')\n",
    "\n",
    "# Fill dataset\n",
    "fit_cut_1_info_tofile[:] = info_m500_cut_1\n",
    "fit_cut_2_info_tofile[:] = info_m500_cut_2\n",
    "\n",
    "file_m500.close()\n",
    "\n",
    "\n",
    "# Create dataset and classes to store important information\n",
    "file_m300 = h5py.File(save_path + 'AE_pvalues_from_fit_info_m300.hdf5', 'w')\n",
    "\n",
    "# Save mse distribution and AUC\n",
    "fit_cut_1_info_tofile = file_m300.create_dataset('fit_m300_cut_1', info_m300_cut_1.shape, dtype='f8')\n",
    "fit_cut_2_info_tofile = file_m300.create_dataset('fit_m300_cut_2', info_m300_cut_2.shape, dtype='f8')\n",
    "\n",
    "# Fill dataset\n",
    "fit_cut_1_info_tofile[:] = info_m300_cut_1\n",
    "fit_cut_2_info_tofile[:] = info_m300_cut_2\n",
    "\n",
    "file_m300.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
