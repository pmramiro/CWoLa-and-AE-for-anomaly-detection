{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py    \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import nbimporter\n",
    "import gc\n",
    "\n",
    "from keras.layers import Input, Dense, Dropout, Average, LeakyReLU\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K \n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.ticker import LogFormatterSciNotation as LogFormatter\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from check_efficiency_after_epoch_end import IntervalEvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare GPU environment and define amount of memory to use\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"  # specify which GPU(s) to be used\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.42)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input features from file\n",
    "features_filename = 'final_features_cwola_and_ae/all_input_features.hdf5'\n",
    "h5_results = h5py.File(features_filename, 'r')\n",
    "\n",
    "# Store features info\n",
    "signal = h5_results['signal'][:]\n",
    "h5_results.close()\n",
    "\n",
    "# Load extra background\n",
    "filename = \"final_features_cwola_and_ae/all_input_features_extra_QCD.hdf5\"\n",
    "h5_results = h5py.File(filename, 'r')\n",
    "\n",
    "# Store reconstruction info\n",
    "background = h5_results['background'][:]\n",
    "\n",
    "h5_results.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shape of data arrays\n",
    "print('Signal features shape: %s' % (signal.shape,))\n",
    "print('Background features shape: %s' % (background.shape,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the amount of memory used by the arrays\n",
    "print('Memory used by signal array: %.2f MB' % (signal.nbytes / (10**3 * 1024)))\n",
    "print('Memory used by background array: %.2f MB' % (background.nbytes / (10**3 * 1024)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print sample signal and background event to make sure everything is fine\n",
    "print('Signal sample event:\\n %s' % signal[-1])\n",
    "print('Background sample event:\\n %s' % background[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study each feature of jet 1 before preprocessing data\n",
    "print('BACKGROUND:')\n",
    "print('mj: (min, max) = (%f, %f) | Only %i below 1' % (background[:,0].min(), background[:,0].max(), background[:,0][background[:,0] < 1].shape[0]))\n",
    "print('t2: (min, max) = (%f, %f)   | Only %i above 15' % (background[:,1].min(), background[:,1].max(), background[:,1][background[:,1] > 15].shape[0]))\n",
    "print('tau21: (min, max) = (%f, %f)' % (background[:,2].min(), background[:,2].max()))\n",
    "print('tau32: (min, max) = (%f, %f)' % (background[:,3].min(), background[:,3].max()))\n",
    "print('tau43: (min, max) = (%f, %f)' % (background[:,4].min(), background[:,4].max()))\n",
    "print('ntrk: (min, max) = (%i, %i)' % (background[:,5].min(), background[:,5].max()))\n",
    "\n",
    "# Study each feature of jet 1 before preprocessing data\n",
    "print('SIGNAL:')\n",
    "print('mj: (min, max) = (%f, %f) | Only %i below 1' % (signal[:,0].min(), signal[:,0].max(), signal[:,0][signal[:,0] < 1].shape[0]))\n",
    "print('t2: (min, max) = (%f, %f)   | Only %i above 15' % (signal[:,1].min(), signal[:,1].max(), signal[:,1][signal[:,1] > 15].shape[0]))\n",
    "print('tau21: (min, max) = (%f, %f)' % (signal[:,2].min(), signal[:,2].max()))\n",
    "print('tau32: (min, max) = (%f, %f)' % (signal[:,3].min(), signal[:,3].max()))\n",
    "print('tau43: (min, max) = (%f, %f)' % (signal[:,4].min(), signal[:,4].max()))\n",
    "print('ntrk: (min, max) = (%i, %i)' % (signal[:,5].min(), signal[:,5].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study each feature of jet 2 before preprocessing data\n",
    "print('BACKGROUND:')\n",
    "print('mj: (min, max) = (%f, %f) | Only %i below 0' % (background[:,9].min(), background[:,9].max(), background[:,9][background[:,9] < 0].shape[0]))\n",
    "print('t2: (min, max) = (%f, %f)' % (background[:,10].min(), background[:,10].max()))\n",
    "print('tau21: (min, max) = (%f, %f)' % (background[:,11].min(), background[:,11].max()))\n",
    "print('tau32: (min, max) = (%f, %f)' % (background[:,12].min(), background[:,12].max()))\n",
    "print('tau43: (min, max) = (%f, %f)' % (background[:,13].min(), background[:,13].max()))\n",
    "print('ntrk: (min, max) = (%i, %i)' % (background[:,14].min(), background[:,14].max()))\n",
    "\n",
    "# Study each feature of jet 2 before preprocessing data\n",
    "print('SIGNAL:')\n",
    "print('mj: (min, max) = (%f, %f) | Only %i below 0' % (signal[:,9].min(), signal[:,9].max(), signal[:,9][signal[:,9] < 0].shape[0]))\n",
    "print('t2: (min, max) = (%f, %f)' % (signal[:,10].min(), signal[:,10].max()))\n",
    "print('tau21: (min, max) = (%f, %f)' % (signal[:,11].min(), signal[:,11].max()))\n",
    "print('tau32: (min, max) = (%f, %f)' % (signal[:,12].min(), signal[:,12].max()))\n",
    "print('tau43: (min, max) = (%f, %f)' % (signal[:,13].min(), signal[:,13].max()))\n",
    "print('ntrk: (min, max) = (%i, %i)' % (signal[:,14].min(), signal[:,14].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Let's make some very simple plots.\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "all_back = np.concatenate((background[:,0], background[:,9]),axis=0)\n",
    "all_sig = np.concatenate((signal[:,0], signal[:,9]),axis=0)\n",
    "plt.hist(all_back, range = (0, 1000), \n",
    "         bins=50, facecolor='r', alpha=0.2,label='background')\n",
    "plt.hist(all_sig, range = (0, 1000), \n",
    "         bins=50, facecolor='b', alpha=0.2,label='signal')\n",
    "plt.xlabel(r'$m_{J}$')\n",
    "plt.ylabel('Relative ocurrence')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlim(xmax=1000)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "all_back = np.concatenate((background[:,1], background[:,10]),axis=0)\n",
    "all_sig = np.concatenate((signal[:,1], signal[:,10]),axis=0)\n",
    "plt.hist(all_back, range = (0, 8), \n",
    "         bins=100, facecolor='r', alpha=0.2,label='background')\n",
    "plt.hist(all_sig, range = (0, 8), \n",
    "         bins=100, facecolor='b', alpha=0.2,label='signal')\n",
    "plt.xlabel(r'$\\sqrt{\\tau_{1}^{(2)}} / \\tau_{1}^{(1)}$')\n",
    "plt.ylabel('Relative ocurrence')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlim(xmin=-0.05, xmax=8)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "all_back = np.concatenate((background[:,2], background[:,11]),axis=0)\n",
    "all_sig = np.concatenate((signal[:,2], signal[:,11]),axis=0)\n",
    "plt.hist(all_back, range = (0, 1), \n",
    "         bins=60, facecolor='r', alpha=0.2,label='background')\n",
    "plt.hist(all_sig, range = (0, 1), \n",
    "         bins=60, facecolor='b', alpha=0.2,label='signal')\n",
    "plt.xlabel(r'$\\tau_{21}$')\n",
    "plt.ylabel('Relative ocurrence')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlim(xmin=-0.05, xmax=1)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "all_back = np.concatenate((background[:,3], background[:,12]),axis=0)\n",
    "all_sig = np.concatenate((signal[:,3], signal[:,12]),axis=0)\n",
    "plt.hist(all_back, range = (0, 1), \n",
    "         bins=60, facecolor='r', alpha=0.2,label='background')\n",
    "plt.hist(all_sig, range = (0, 1), \n",
    "         bins=60, facecolor='b', alpha=0.2,label='signal')\n",
    "plt.xlabel(r'$\\tau_{32}$')\n",
    "plt.ylabel('Relative ocurrence')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlim(xmin=-0.05, xmax=1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "all_back = np.concatenate((background[:,4], background[:,13]),axis=0)\n",
    "all_sig = np.concatenate((signal[:,4], signal[:,13]),axis=0)\n",
    "plt.hist(all_back, range = (0, 1), \n",
    "         bins=60, facecolor='r', alpha=0.2,label='background')\n",
    "plt.hist(all_sig, range = (0, 1), \n",
    "         bins=60, facecolor='b', alpha=0.2,label='signal')\n",
    "plt.xlabel(r'$\\tau_{43}$')\n",
    "plt.ylabel('Relative ocurrence')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlim(xmax=1)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "all_back = np.concatenate((background[:,5], background[:,14]),axis=0)\n",
    "all_sig = np.concatenate((signal[:,5], signal[:,14]),axis=0)\n",
    "plt.hist(all_back, range = (0, 200), \n",
    "         bins=50, facecolor='r', alpha=0.2,label='background')\n",
    "plt.hist(all_sig, range = (0, 200), \n",
    "         bins=50, facecolor='b', alpha=0.2,label='signal')\n",
    "plt.xlabel(r'$n_{trk}$')\n",
    "plt.ylabel('Relative ocurrence')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check background distribution on mj1 and mj2, since we might be able to get rid of some \"outliers\" or weird back.\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15.1, 6))\n",
    "plt.subplots_adjust(wspace = 0.3)\n",
    "\n",
    "# Prepare inputs\n",
    "mj1_background = background[:,0]\n",
    "mj2_background = background[:,9]\n",
    "\n",
    "h1 = ax1.hist2d(mj1_background, mj2_background, bins=(100, 100), range=[[10, 1000], [10, 600]], norm=LogNorm(), cmap=plt.cm.jet, label='background')\n",
    "divider = make_axes_locatable(ax1)  # create an axes on the right side of ax.\n",
    "cax = divider.append_axes(\"right\", size=\"8%\", pad=0.1)\n",
    "cb = plt.colorbar(h1[3], ax=ax1, cax = cax, format=LogFormatter(10))\n",
    "ax1.set_xlabel(r'$m_{J1}$ [GeV]') \n",
    "ax1.set_ylabel(r'$m_{J2}$ [GeV]')\n",
    "ax1.set_xlim(xmin=10, xmax=1000)\n",
    "ax1.set_ylim(ymin=10, ymax=600)\n",
    "\n",
    "\n",
    "mj1_signal = signal[:,0]\n",
    "mj2_signal = signal[:,9]\n",
    "\n",
    "h2 = ax2.hist2d(mj1_signal, mj2_signal, bins=(100, 100), norm=LogNorm(), cmap=plt.cm.jet, label='signal')\n",
    "divider = make_axes_locatable(ax2)  # create an axes on the right side of ax.\n",
    "cax = divider.append_axes(\"right\", size=\"8%\", pad=0.1)\n",
    "cb = plt.colorbar(h1[3], ax=ax2, cax = cax, format=LogFormatter(10))\n",
    "ax2.set_xlabel(r'$m_{J1}$ [GeV]') \n",
    "ax2.set_ylabel(r'$m_{J2}$ [GeV]')\n",
    "ax2.set_xlim(xmin=10, xmax=1000)\n",
    "ax2.set_ylim(ymin=10, ymax=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only a few input features\n",
    "back_1 = background[:,0].reshape((1,len(background)))    # mj (1)\n",
    "back_2 = background[:,1].reshape((1,len(background)))    # t2 (1)\n",
    "back_3 = background[:,2].reshape((1,len(background)))    # tau21 (1)\n",
    "back_4 = background[:,3].reshape((1,len(background)))    # tau32 (1)\n",
    "back_5 = background[:,4].reshape((1,len(background)))    # tau43 (1)\n",
    "back_6 = background[:,5].reshape((1,len(background)))    # ntrk (1)\n",
    "\n",
    "back_7 = background[:,9].reshape((1,len(background)))    # mj (2)\n",
    "back_8 = background[:,10].reshape((1,len(background)))   # t2 (2)\n",
    "back_9 = background[:,11].reshape((1,len(background)))   # tau21 (2)\n",
    "back_10 = background[:,12].reshape((1,len(background)))  # tau32 (2)\n",
    "back_11 = background[:,13].reshape((1,len(background)))  # tau43 (2)\n",
    "back_12 = background[:,14].reshape((1,len(background)))  # ntrk (2)\n",
    "\n",
    "back_mjj = background[:,18].reshape((1,len(background)))\n",
    "\n",
    "sig_1 = signal[:,0].reshape((1,len(signal)))    # mj (1)\n",
    "sig_2 = signal[:,1].reshape((1,len(signal)))    # t2 (1)\n",
    "sig_3 = signal[:,2].reshape((1,len(signal)))    # tau21 (1)\n",
    "sig_4 = signal[:,3].reshape((1,len(signal)))    # tau32 (1)\n",
    "sig_5 = signal[:,4].reshape((1,len(signal)))    # tau43 (1)\n",
    "sig_6 = signal[:,5].reshape((1,len(signal)))    # ntrk (1)\n",
    "\n",
    "sig_7 = signal[:,9].reshape((1,len(signal)))    # mj (2)\n",
    "sig_8 = signal[:,10].reshape((1,len(signal)))   # t2 (2)\n",
    "sig_9 = signal[:,11].reshape((1,len(signal)))   # tau21 (2)\n",
    "sig_10 = signal[:,12].reshape((1,len(signal)))  # tau32 (2)\n",
    "sig_11 = signal[:,13].reshape((1,len(signal)))  # tau43 (2)\n",
    "sig_12 = signal[:,14].reshape((1,len(signal)))  # ntrk (2)\n",
    "\n",
    "sig_mjj = signal[:,18].reshape((1,len(signal)))\n",
    "\n",
    "# Input features: [mj1, t2(1), tau21(1), tau32(1), tau43(1), ntrk1] \n",
    "#                 [mj2, t2(2), tau21(2), tau32(2), tau43(2), ntrk2, mjj]\n",
    "\n",
    "background = np.concatenate((back_1, back_2, back_3, back_4, back_5, back_6, back_7, back_8, back_9, back_10, back_11, back_12, back_mjj), axis=0).T\n",
    "signal = np.concatenate((sig_1, sig_2, sig_3, sig_4, sig_5, sig_6, sig_7, sig_8, sig_9, sig_10, sig_11, sig_12, sig_mjj), axis=0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print sample signal and background event to make sure everything is fine\n",
    "print('Background features shape before mj cut: %s' % (background.shape,))\n",
    "print('Signal features shape before mj cut: %s' % (signal.shape,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input features: [mj1, t2(1), tau21(1), tau32(1), tau43(1), ntrk1] \n",
    "#                 [mj2, t2(2), tau21(2), tau32(2), tau43(2), ntrk2, mjj]\n",
    "\n",
    "##################################### mj #####################################\n",
    "# Plot the reduced set of input features that we use to fee the AE\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "all_back = background[:,0]\n",
    "all_sig = signal[:,0]\n",
    "plt.hist(all_back, range = (0, 1600), \n",
    "         bins=60, facecolor='r', alpha=0.2,label='background')\n",
    "plt.hist(all_sig, range = (0, 1600), \n",
    "         bins=60, facecolor='b', alpha=0.2,label='signal')\n",
    "plt.xlabel(r'$m_{J1}$')\n",
    "plt.ylabel('Relative ocurrence')\n",
    "plt.legend(loc='upper right')\n",
    "plt.semilogy()\n",
    "plt.xlim(xmax=1600)\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "all_back = background[:,6]\n",
    "all_sig = signal[:,6]\n",
    "plt.hist(all_back, range = (0, 1600), \n",
    "         bins=60, facecolor='r', alpha=0.2,label='background')\n",
    "plt.hist(all_sig, range = (0, 1600), \n",
    "         bins=60, facecolor='b', alpha=0.2,label='signal')\n",
    "plt.xlabel(r'$m_{J2}$')\n",
    "plt.ylabel('Relative ocurrence')\n",
    "plt.legend(loc='upper right')\n",
    "plt.semilogy()\n",
    "plt.xlim(xmax=1600)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "##################################### t2 #####################################\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "all_back = background[:,1]\n",
    "all_sig = signal[:,1]\n",
    "plt.hist(all_back, range = (0, 8), \n",
    "         bins=100, facecolor='r', alpha=0.2,label='background')\n",
    "plt.hist(all_sig, range = (0, 8), \n",
    "         bins=100, facecolor='b', alpha=0.2,label='signal')\n",
    "plt.xlabel(r'$\\sqrt{\\tau_{1}^{(2)}} / \\tau_{1}^{(1)}$')\n",
    "plt.ylabel('Relative ocurrence')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlim(xmin=-0.05, xmax=8)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "all_back = background[:,7]\n",
    "all_sig = signal[:,7]\n",
    "plt.hist(all_back, range = (0, 8), \n",
    "         bins=100, facecolor='r', alpha=0.2,label='background')\n",
    "plt.hist(all_sig, range = (0, 8), \n",
    "         bins=100, facecolor='b', alpha=0.2,label='signal')\n",
    "plt.xlabel(r'$\\sqrt{\\tau_{1}^{(2)}} / \\tau_{1}^{(1)}$')\n",
    "plt.ylabel('Relative ocurrence')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlim(xmin=-0.05, xmax=8)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "##################################### tau21 #####################################\n",
    "\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "all_back = background[:,2]\n",
    "all_sig = signal[:,2]\n",
    "plt.hist(all_back, range = (0, 1), \n",
    "         bins=60, facecolor='r', alpha=0.2,label='background')\n",
    "plt.hist(all_sig, range = (0, 1), \n",
    "         bins=60, facecolor='b', alpha=0.2,label='signal')\n",
    "plt.xlabel(r'$\\tau_{21} (1)$')\n",
    "plt.ylabel('Relative ocurrence')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlim(xmin=-0.05, xmax=1)\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "all_back = background[:,8]\n",
    "all_sig = signal[:,8]\n",
    "plt.hist(all_back, range = (0, 1), \n",
    "         bins=60, facecolor='r', alpha=0.2,label='background')\n",
    "plt.hist(all_sig, range = (0, 1), \n",
    "         bins=60, facecolor='b', alpha=0.2,label='signal')\n",
    "plt.xlabel(r'$\\tau_{21} (2)$')\n",
    "plt.ylabel('Relative ocurrence')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlim(xmin=-0.05, xmax=1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "##################################### tau32 #####################################\n",
    "\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "all_back = background[:,3]\n",
    "all_sig = signal[:,3]\n",
    "plt.hist(all_back, range = (0, 1), \n",
    "         bins=60, facecolor='r', alpha=0.2,label='background')\n",
    "plt.hist(all_sig, range = (0, 1), \n",
    "         bins=60, facecolor='b', alpha=0.2,label='signal')\n",
    "plt.xlabel(r'$\\tau_{32} (1)$')\n",
    "plt.ylabel('Relative ocurrence')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlim(xmin=-0.05, xmax=1)\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "all_back = background[:,9]\n",
    "all_sig = signal[:,9]\n",
    "plt.hist(all_back, range = (0, 1), \n",
    "         bins=60, facecolor='r', alpha=0.2,label='background')\n",
    "plt.hist(all_sig, range = (0, 1), \n",
    "         bins=60, facecolor='b', alpha=0.2,label='signal')\n",
    "plt.xlabel(r'$\\tau_{32} (2)$')\n",
    "plt.ylabel('Relative ocurrence')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlim(xmin=-0.05, xmax=1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "##################################### tau43 #####################################\n",
    "\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "all_back = background[:,4]\n",
    "all_sig = signal[:,4]\n",
    "plt.hist(all_back, range = (0, 1), \n",
    "         bins=60, facecolor='r', alpha=0.2,label='background')\n",
    "plt.hist(all_sig, range = (0, 1), \n",
    "         bins=60, facecolor='b', alpha=0.2,label='signal')\n",
    "plt.xlabel(r'$\\tau_{43} (1)$')\n",
    "plt.ylabel('Relative ocurrence')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlim(xmin=-0.05, xmax=1)\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "all_back = background[:,10]\n",
    "all_sig = signal[:,10]\n",
    "plt.hist(all_back, range = (0, 1), \n",
    "         bins=60, facecolor='r', alpha=0.2,label='background')\n",
    "plt.hist(all_sig, range = (0, 1), \n",
    "         bins=60, facecolor='b', alpha=0.2,label='signal')\n",
    "plt.xlabel(r'$\\tau_{43} (2)$')\n",
    "plt.ylabel('Relative ocurrence')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlim(xmin=-0.05, xmax=1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "##################################### ntrk #####################################\n",
    "\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "all_back = background[:,5]\n",
    "all_sig = signal[:,5]\n",
    "plt.hist(all_back, range = (0, 200), \n",
    "         bins=50, facecolor='r', alpha=0.2,label='background')\n",
    "plt.hist(all_sig, range = (0, 200), \n",
    "         bins=50, facecolor='b', alpha=0.2,label='signal')\n",
    "plt.xlabel(r'$n_{trk}(1)$')\n",
    "plt.ylabel('Relative ocurrence')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "all_back = background[:,11]\n",
    "all_sig = signal[:,11]\n",
    "plt.hist(all_back, range = (0, 200), \n",
    "         bins=50, facecolor='r', alpha=0.2,label='background')\n",
    "plt.hist(all_sig, range = (0, 200), \n",
    "         bins=50, facecolor='b', alpha=0.2,label='signal')\n",
    "plt.xlabel(r'$n_{trk}(2)$')\n",
    "plt.ylabel('Relative ocurrence')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to shuffle array elements in unison\n",
    "def shuffle_2D(a, b):\n",
    "    n_elem = a.shape[0]\n",
    "    indeces = np.random.choice(n_elem, size=n_elem, replace=False)\n",
    "    return a[indeces], b[indeces]\n",
    "\n",
    "def shuffle_3D(a, b, c):\n",
    "    n_elem = a.shape[0]\n",
    "    indeces = np.random.choice(n_elem, size=n_elem, replace=False)\n",
    "    return a[indeces], b[indeces], c[indeces]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################\n",
    "###################################################################################################################\n",
    "#################################################                 #################################################\n",
    "#################################################    RUN CWOLA    #################################################\n",
    "#################################################                 #################################################\n",
    "###################################################################################################################\n",
    "###################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of signal and background events for the S/B scan. S/B benchmarks denotes as B1-B7\n",
    "back_scan = [537304, 537304, 537304, 537304, 537304, 537304, 537304]\n",
    "sig_scan = [730, 580, 440, 350, 265, 175, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define run number (run this same notebook in \"parallel\", files will be saved with suffix \"_run\")\n",
    "run = 1\n",
    "\n",
    "# Create folders to store the trained models\n",
    "for i in range(1, 1+len(back_scan)):\n",
    "    !mkdir CWoLa_signal_m500_models/models_B{i}_%d % run              # create folder to store trained models\n",
    "    !mkdir CWoLa_signal_m500_models/ensemble_models_B{i}_%d % run     # create folder to store ensemble models\n",
    "    !mkdir CWoLa_signal_m500_models/extra_info_B{i}_%d % run          # create folder to store file with some info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove signal events outside the range 2800 < mjj < 5200. Note: the initial sample of background events has events\n",
    "# in the range 1000 < mJJ < 8000, and peaks at ~2700 GeV. The signal distribution is centered around ~ 3500 GeV. As\n",
    "# a consequence, we will only consider events in the relevant mJJ range: 2800 < mJJ < 5200 works well for us.\n",
    "signal = signal[(signal[:,12] > mass_min) & (signal[:,12] < mass_max)]\n",
    "background = background[(background[:,12] > mass_min) & (background[:,12] < mass_max)]\n",
    "\n",
    "# Make copies of these arrays to take random subsamples of events later on\n",
    "signal_ref = np.copy(signal)\n",
    "background_ref = np.copy(background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "############################################   TRAINING   ############################################\n",
    "######################################################################################################\n",
    "\n",
    "# Define training variables\n",
    "kfolds = 5              # number of kfolds for the nested cross-validations procedure\n",
    "ntries = 10             # number of models to train in each cross-validation round\n",
    "nepochs = 700           # maximum number of epochs each model is trained for\n",
    "nbatch = int(256*80)    # batch size\n",
    "patience = 300          # wait n = patience epochs to see if validation performance improves before ending training\n",
    "eff_rate = 0.01         # custom validation metric: signal region efficiency\n",
    "\n",
    "# Define other variables\n",
    "nbin = 30               # number of bins that we consider in the 2800 < mJJ < 5200\n",
    "counter = 1             # this variable is only defined to keep track of the S/B benchmark and print some info\n",
    "\n",
    "\n",
    "# Loop over S/B benchmarks\n",
    "for evs in range(len(back_scan)):\n",
    "    \n",
    "    print('******************************  Training Benchmark %d  ******************************' % counter)\n",
    "    \n",
    "    # Shuffle input arrays\n",
    "    signal = np.random.permutation(signal_ref)\n",
    "    background = np.random.permutation(background_ref)\n",
    "    \n",
    "    # Define number of events for this benchmark\n",
    "    n_signal = sig_scan[evs]                       # number of signal events used for training\n",
    "    n_background = back_scan[evs]                  # number of background events used for training\n",
    "    n_signal_extra = len(signal_ref) - n_signal    # the rest of signal events are used to compute the AUC metric\n",
    "\n",
    "    # Define signal and background events\n",
    "    signal_extra = signal[n_signal:(n_signal+n_signal_extra)]\n",
    "    signal = signal[:n_signal]\n",
    "    background = background[:n_background]\n",
    "    \n",
    "    print('Background features shape: %s' % (background.shape,))\n",
    "    print('Signal features shape: %s' % (signal.shape,))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    #######################################################################################################\n",
    "    ##########################################   Prepare input   ##########################################\n",
    "    #######################################################################################################\n",
    "    \n",
    "    # For CWoLa, we will need k-folds. In this part of the code, we define an array with all the signal and \n",
    "    # background events that are used for training. We define the signal region (SR) and the sideband region (SB),\n",
    "    # bin all data and split it in kfolds.\n",
    "\n",
    "    # Define signal and background labels\n",
    "    signal_labels = np.ones(len(signal))\n",
    "    background_labels = np.zeros(len(background))\n",
    "    \n",
    "    # Mix signal and background inputs, add truth label as an extra column at the end\n",
    "    X = np.concatenate((signal, background), axis=0)\n",
    "    truth_labels = np.concatenate((signal_labels, background_labels), axis=0)\n",
    "    truth_labels = truth_labels.reshape(truth_labels.shape[0], 1)\n",
    "    X = np.concatenate((X, truth_labels.reshape(truth_labels.shape[0], 1)), axis=1)\n",
    "\n",
    "    # Define extra signal labels\n",
    "    signal_extra_labels = np.ones(len(signal_extra))\n",
    "    truth_labels = signal_extra_labels.reshape(signal_extra_labels.shape[0], 1)\n",
    "    signal_extra = np.concatenate((signal_extra, truth_labels), axis=1)\n",
    "\n",
    "    # Find signal peak. If signal sample is large, calculate mean, otherwise use mean value from the full sample\n",
    "    if (len(signal) > 100):\n",
    "        mpeak = np.mean(signal[:,12], axis=0)\n",
    "    else:\n",
    "        mpeak = 3507.1692\n",
    "    \n",
    "    # Divide data in 30 bins of log(mjj) uniformly\n",
    "    bins = np.logspace(np.log10(mass_min), np.log10(mass_max), num=nbin+1)\n",
    "    bins_centers = np.array([0.5*(bins[i] + bins[i+1]) for i in range(0,len(bins)-1)])\n",
    "    SR_lower_edge = 9     # index of \"bins\" array where the SR lower edge is stored\n",
    "    SR_upper_edge = 13    # index of \"bins\" array where the SR upper edge is stored\n",
    "    SB_lower_edge = 6     # index of \"bins\" array where the low SB lower edge is stored\n",
    "    SB_upper_edge = 16    # index of \"bins\" array where the high SB upper edge is stored\n",
    "\n",
    "    # Prepare data for cross-validation\n",
    "    bin_list_kfolds = []  # bin_list_kfolds[bin number][number of kfolds, kfold size, number of features]\n",
    "\n",
    "    for i in range(nbin):\n",
    "        \n",
    "        # Take all the events in a given bin\n",
    "        w = X[(X[:,12] >= bins[i]) & (X[:,12] < bins[i+1])]\n",
    "        w = np.random.permutation(w)\n",
    "        \n",
    "        # Split the list in kfolds of equal length\n",
    "        kfold_size = int(len(w)/5)\n",
    "        bin_list_kfolds.append(np.zeros((5, kfold_size, 14)))\n",
    "        \n",
    "        # Split all the events in a given bin in kfolds\n",
    "        for j in range(kfolds):\n",
    "            bin_list_kfolds[i][j] = w[j*kfold_size:(j+1)*kfold_size]\n",
    "            \n",
    "        pass\n",
    "    \n",
    "    \n",
    "    #######################################################################################################\n",
    "    ##########################################   Calculate S/B   ##########################################\n",
    "    #######################################################################################################       \n",
    "    \n",
    "    # Compute S/B in SR\n",
    "    dim_back = 0   # count total number of background events in the SR\n",
    "    dim_sig = 0    # count total number of signal events in the SR\n",
    "\n",
    "    for i in range(SR_lower_edge, SR_upper_edge):\n",
    "        for k in range (kfolds):\n",
    "            for j in range(bin_list_kfolds[i][k].shape[0]):\n",
    "    \n",
    "                if (bin_list_kfolds[i][k][j][13] == 0):\n",
    "                    dim_back += 1\n",
    "                else:\n",
    "                    dim_sig += 1\n",
    "    \n",
    "    # Calculate useful information\n",
    "    S_B = dim_sig/dim_back                  # S/B in the SR\n",
    "    S_sqrt_B = dim_sig/np.sqrt(dim_back)    # S/sqrt(B) in the SR\n",
    "    \n",
    "    print('**********  Information  **********')\n",
    "    print('Background events in SR: %d' % dim_back)\n",
    "    print('Signal events in SR: %d' % dim_sig)\n",
    "    print('S/sqrt(B) in SR: %.3f' % S_sqrt_B)\n",
    "    print('S/B in SR: %.5f' % S_B)\n",
    "    print('Signal events peak at m_JJ = %.0f GeV' % mpeak)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    #######################################################################################################\n",
    "    ############################################   Training   #############################################\n",
    "    #######################################################################################################\n",
    "    \n",
    "    # Define lists to store useful information\n",
    "    best_val = [[[] for l in range(kfolds-1)] for k in range(kfolds)]        # store signal efficiency on val data\n",
    "    models = [[[] for l in range(kfolds-1)] for k in range(kfolds)]          # store trained models\n",
    "    models_history = [[[] for l in range(kfolds-1)] for k in range(kfolds)]  # store trained models history\n",
    "    X_test_list = []                                                         # store test data fold in this list\n",
    "    X_test_raw_list = []                                                     # store test raw data fold in this list\n",
    "\n",
    "    # Implement cross-validation procedure\n",
    "    for k in range(kfolds):                   # Loop over test fold\n",
    "        for l in range(kfolds):               # Loop over validation fold\n",
    "            \n",
    "            #######################################################\n",
    "            #####  Define training, validation and test sets  #####\n",
    "            #######################################################\n",
    "            \n",
    "            # If both folds are \"the same\", go next (i.e. training and validation fold must be different)\n",
    "            if (l == k):\n",
    "                continue\n",
    "        \n",
    "            # Define lists to store events used for training, validation and testing in a given cross-validation \n",
    "            # round. Each list will have the following structure: \n",
    "            # list[bin number][kfold number, kfold size, number of features]\n",
    "            train = []\n",
    "            val = []\n",
    "            test = []\n",
    "            \n",
    "            # Split data in each bin in n = kfolds parts, and add each part to one of the previously defined lists\n",
    "            for i in range(nbin):\n",
    "    \n",
    "                # Initialize array in each list\n",
    "                kfold_size = bin_list_kfolds[i].shape[1]\n",
    "            \n",
    "                train.append(np.zeros((3, kfold_size, 14)))\n",
    "                val.append(np.zeros((1, kfold_size, 14)))\n",
    "                test.append(np.zeros((1, kfold_size, 14)))\n",
    "            \n",
    "                # Select training data\n",
    "                train_folds = [j for j in range(kfolds)]   # define index of the training folds\n",
    "                train_folds.remove(k)                      # remove index of the testing fold\n",
    "                train_folds.remove(l)                      # remove index of the validation fold\n",
    "            \n",
    "                # Fill arrays with data\n",
    "                train[i] = np.array([bin_list_kfolds[i][j] for j in train_folds]).reshape((train[i].shape[0]*train[i].shape[1], 14))\n",
    "                val[i] = bin_list_kfolds[i][l].reshape((val[i].shape[0]*val[i].shape[1], 14))\n",
    "                test[i] = bin_list_kfolds[i][k].reshape((test[i].shape[0]*test[i].shape[1], 14))\n",
    "\n",
    "                \n",
    "            ################################################################\n",
    "            #####  Define SR and SB using the previously defined sets  #####\n",
    "            ################################################################\n",
    "\n",
    "            # Define SR and short SB (lower and upper sidebands are defined separately, then joined together)\n",
    "            SR_train = np.concatenate((train[SR_lower_edge], train[SR_lower_edge+1], train[SR_lower_edge+2], train[SR_lower_edge+3]), axis=0)\n",
    "            SB_low_train = np.concatenate((train[SB_lower_edge], train[SB_lower_edge+1], train[SB_lower_edge+2]), axis=0)\n",
    "            SB_up_train = np.concatenate((train[SR_upper_edge], train[SR_upper_edge+1], train[SR_upper_edge+2]), axis=0)\n",
    "            SB_train = np.concatenate((SB_low_train, SB_up_train), axis=0)\n",
    "        \n",
    "            SR_val = np.concatenate((val[SR_lower_edge], val[SR_lower_edge+1], val[SR_lower_edge+2], val[SR_lower_edge+3]), axis=0)\n",
    "            SB_low_val = np.concatenate((val[SB_lower_edge], val[SB_lower_edge+1], val[SB_lower_edge+2]), axis=0)\n",
    "            SB_up_val = np.concatenate((val[SR_upper_edge], val[SR_upper_edge+1], val[SR_upper_edge+2]), axis=0)\n",
    "            SB_val = np.concatenate((SB_low_val, SB_up_val), axis=0)\n",
    "        \n",
    "            # We train on SR + short SB and test on all bins\n",
    "            SR_test = np.concatenate((test[SR_lower_edge], test[SR_lower_edge+1], test[SR_lower_edge+2], test[SR_lower_edge+3]), axis=0)\n",
    "            SB_low_test = test[0]\n",
    "            SB_up_test = test[SR_upper_edge]\n",
    "        \n",
    "            for m in range(1, SR_lower_edge):\n",
    "                SB_low_test = np.concatenate((SB_low_test, test[m]), axis=0)\n",
    "                \n",
    "            for m in range(SR_upper_edge+1, nbin): \n",
    "                SB_up_test = np.concatenate((SB_up_test, test[m]), axis=0)\n",
    "            \n",
    "            SB_test = np.concatenate((SB_low_test, SB_up_test), axis=0)\n",
    "\n",
    "            # Define number of events in SR and SB for the training, validation and test sets\n",
    "            N_SR_train, N_low_train, N_up_train = len(SR_train), len(SB_low_train), len(SB_up_train)\n",
    "            N_SR_val, N_low_val, N_up_val = len(SR_val), len(SB_low_val), len(SB_up_val)\n",
    "            N_SR_test, N_low_test, N_up_test = len(SR_test), len(SB_low_test), len(SB_up_test)\n",
    "\n",
    "            # Define total number of events used for training, validation and testing\n",
    "            N_train = len(SR_train) + len(SB_low_train) + len(SB_up_train)\n",
    "            N_val = len(SR_val) + len(SB_low_val) + len(SB_up_val)\n",
    "            N_test = len(SR_test) + len(SB_low_test) + len(SB_up_test)\n",
    "        \n",
    "            # Calculate weights\n",
    "            w_SR_train = (2*N_train) / (4*N_SR_train)\n",
    "            w_low_train = N_train / (4*N_low_train)\n",
    "            w_up_train = N_train / (4*N_up_train)\n",
    "        \n",
    "            w_SR_val = (2*N_val) / (4*N_SR_val)\n",
    "            w_low_val = N_val / (4*N_low_val)\n",
    "            w_up_val = N_val / (4*N_up_val)\n",
    "        \n",
    "            w_SR_test = (2*N_test) / (4*N_SR_test)\n",
    "            w_low_test = N_test / (4*N_low_test)\n",
    "            w_up_test = N_test / (4*N_up_test)\n",
    "        \n",
    "            # Define arrays with the weights\n",
    "            SR_train_weights = np.ones(len(SR_train)) * w_SR_train\n",
    "            SB_low_train_weights = np.ones(len(SB_low_train)) * w_low_train\n",
    "            SB_up_train_weights = np.ones(len(SB_up_train)) * w_up_train\n",
    "            SB_train_weights = np.concatenate((SB_low_train_weights, SB_up_train_weights))\n",
    "        \n",
    "            SR_val_weights = np.ones(len(SR_val)) * w_SR_val\n",
    "            SB_low_val_weights = np.ones(len(SB_low_val)) * w_low_val\n",
    "            SB_up_val_weights = np.ones(len(SB_up_val)) * w_up_val\n",
    "            SB_val_weights = np.concatenate((SB_low_val_weights, SB_up_val_weights))\n",
    "        \n",
    "            # Define SR and SB labels (events in the SR labeled as 1, events in the SB labeled as 0)\n",
    "            SR_train_labels = np.ones(len(SR_train))\n",
    "            SB_train_labels = np.zeros(len(SB_train))\n",
    "        \n",
    "            SR_val_labels = np.ones(len(SR_val))\n",
    "            SB_val_labels = np.zeros(len(SB_val))\n",
    "            \n",
    "            \n",
    "            ###################################################################\n",
    "            #####  Build the arrays that we will use to train the models  #####\n",
    "            ################################################################### \n",
    "\n",
    "            # Build input arrays\n",
    "            X_train = np.concatenate((SR_train, SB_train), axis=0)\n",
    "            Y_train = np.concatenate((SR_train_labels, SB_train_labels))\n",
    "            Z_train = np.concatenate((SR_train_weights, SB_train_weights))\n",
    "        \n",
    "            X_val = np.concatenate((SR_val, SB_val), axis=0)\n",
    "            Y_val = np.concatenate((SR_val_labels, SB_val_labels))\n",
    "            Z_val = np.concatenate((SR_val_weights, SB_val_weights))\n",
    "        \n",
    "            X_test = np.concatenate((SR_test, SB_test), axis=0)\n",
    "\n",
    "            # Shuffle input arrays\n",
    "            X_train, Y_train, Z_train = shuffle_3D(X_train, Y_train, Z_train)\n",
    "            X_val, Y_val, Z_val = shuffle_3D(X_val, Y_val, Z_val)\n",
    "            X_test = np.random.permutation(X_test)\n",
    "        \n",
    "    \n",
    "            ############################################################\n",
    "            #####  Preprocess the input features: standardization  #####\n",
    "            ############################################################\n",
    "    \n",
    "            # Keep arrays with raw features\n",
    "            X_train_raw = np.copy(X_train)\n",
    "            X_val_raw = np.copy(X_val)\n",
    "            X_test_raw = np.copy(X_test)\n",
    "        \n",
    "            # Calculate mean and std for each feature\n",
    "            X_test_pre_mean = np.mean(np.concatenate((X_train_raw[:,:12], X_val_raw[:,:12]), axis=0), axis=0)\n",
    "            X_test_pre_std = np.std(np.concatenate((X_train_raw[:,:12], X_val_raw[:,:12]), axis=0), axis=0)\n",
    "\n",
    "            # Standardize input features (i.e. substract mean, divide by standard deviation)\n",
    "            X_train[:,:12] = (X_train[:,:12] - np.mean(X_train_raw[:,:12], axis=0))/np.std(X_train_raw[:,:12], axis=0)\n",
    "            X_val[:,:12] = (X_val[:,:12] - np.mean(X_train_raw[:,:12], axis=0))/np.std(X_train_raw[:,:12], axis=0) \n",
    "            X_test[:,:12] = (X_test[:,:12] - X_test_pre_mean) / X_test_pre_std\n",
    "        \n",
    "        \n",
    "            #########################################################\n",
    "            #####  Training phase: define NN and train models   #####\n",
    "            #########################################################\n",
    "            \n",
    "            # Reset l index to fill arrays\n",
    "            if (l > k):\n",
    "                l = l-1\n",
    "    \n",
    "            # Train n = ntries models on same data and save the one with the best performance on the validation set\n",
    "            for i in range(ntries):\n",
    "            \n",
    "                K.clear_session()\n",
    "            \n",
    "                # Build custom optimizer | Default: lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0\n",
    "                myoptimizer = Adam(lr=0.001, beta_1=0.8, beta_2=0.99, epsilon=1e-08, decay=0.0005)\n",
    "            \n",
    "                # Build NN structure\n",
    "                model = Sequential()\n",
    "                model.add(Dense(64, input_dim=12, activation='relu', use_bias=True,\n",
    "                                bias_initializer = TruncatedNormal(mean=0., stddev=0.04)))\n",
    "                model.add(LeakyReLU(alpha=0.1))\n",
    "                model.add(Dropout(0.2))\n",
    "                model.add(Dense(32, activation='elu', use_bias=True,\n",
    "                                bias_initializer = TruncatedNormal(mean=0., stddev=0.01)))\n",
    "                model.add(Dropout(0.2))\n",
    "                model.add(Dense(16, activation='elu', use_bias=True,\n",
    "                                bias_initializer = TruncatedNormal(mean=0., stddev=0.01)))\n",
    "                model.add(Dropout(0.2))\n",
    "                model.add(Dense(4, activation='elu', use_bias=True,\n",
    "                                bias_initializer = TruncatedNormal(mean=0., stddev=0.01)))\n",
    "                model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "                # Define filename to save the trained model\n",
    "                if (l < k):\n",
    "                    checkpoint_filename = \"CWoLa_signal_m500_models/models_B%d_%d/model_[\" % (counter, run) + str(k) + \",\" + str(l) + \"]_\" + str(i) + \".h5\"\n",
    "                else:\n",
    "                    checkpoint_filename = \"CWoLa_signal_m500_models/models_B%d_%d/model_[\" % (counter, run) + str(k) + \",\" + str(l+1) + \"]_\" + str(i) + \".h5\" \n",
    "\n",
    "                # Monitor the evolution of our custom metric during training (this piece of code was written by Jack)\n",
    "                ival = IntervalEvaluation(training_data=(X_train[:,:12], Y_train), \n",
    "                                          validation_data=(X_val[:,:12], Y_val),\n",
    "                                          verbose=0, filename=checkpoint_filename,\n",
    "                                          eff_rate=eff_rate, patience=patience,\n",
    "                                          min_epoch=10, plot_period=2)\n",
    "                \n",
    "                # Compile and fit model\n",
    "                model.compile(loss='binary_crossentropy', optimizer=myoptimizer)\n",
    "\n",
    "                model_history = model.fit(X_train[:,:12], Y_train, epochs=nepochs, batch_size=nbatch,\n",
    "                                          verbose=0, callbacks=[ival],\n",
    "                                          sample_weight=Z_train, validation_data=(X_val[:,:12], Y_val, Z_val))\n",
    "                \n",
    "                # Save model history\n",
    "                models_history[k][l].append(model_history)\n",
    "                \n",
    "                # Reload the trained model\n",
    "                del model\n",
    "                model = load_model(checkpoint_filename)\n",
    "    \n",
    "                # Calculate scores for training and validation data\n",
    "                scores_train = model.predict(X_train[:,:12], batch_size=nbatch).flatten()\n",
    "                scores_val = model.predict(X_val[:,:12], batch_size=nbatch).flatten()\n",
    "                \n",
    "                # Get score threshold above which only x% of the events survive in the SB (using validation data!)\n",
    "                scores_sorted = np.sort(scores_val[Y_val == 0])[::-1]   # inverse sort, i.e. in descending order\n",
    "                cut = eff_rate * len(scores_sorted)\n",
    "                thresh = scores_sorted[int(cut)]\n",
    "        \n",
    "                # Find fraction of events in the SR which survive a cut on the above threshold (our custom metric!)\n",
    "                scores_sorted = np.sort(scores_val[Y_val == 1])\n",
    "                sig_eff_val = 1.0 - 1.0*np.searchsorted(scores_sorted, thresh) / len(scores_sorted)\n",
    "        \n",
    "                # Save the signal efficiency\n",
    "                best_val[k][l].append(sig_eff_val)\n",
    "    \n",
    "                # Define filename to save the trained model with the best validation performance\n",
    "                if (l < k):\n",
    "                    best_model_filename = \"CWoLa_signal_m500_models/models_B%d_%d/best_[\" % (counter, run) + str(k) + \",\" + str(l) + \"]_\" + str(i) + \".h5\"\n",
    "                else:\n",
    "                    best_model_filename = \"CWoLa_signal_m500_models/models_B%d_%d/best_[\" % (counter, run) + str(k) + \",\" + str(l+1) + \"]_\" + str(i) + \".h5\"\n",
    "            \n",
    "                # If this model is the best so far, save it\n",
    "                if (len(best_val[k][l]) == 0):\n",
    "                    model.save(best_model_filename)\n",
    "                elif (sig_eff_val >= np.array(best_val[k][l]).max()):\n",
    "                    model.save(best_model_filename)\n",
    "\n",
    "                # Clear memory\n",
    "                K.clear_session()\n",
    "                gc.collect()\n",
    "                del model\n",
    "            \n",
    "                pass  # End ntries loop\n",
    "        \n",
    "            pass  # End l loop\n",
    "        \n",
    "    \n",
    "        # Pick the best model from the ntries for each validation kfold, and save them together in a list\n",
    "        best_val_models = []\n",
    "        best_val_models_hist = []\n",
    "        \n",
    "        for l in range(kfolds):\n",
    "            if (l == k):\n",
    "                continue\n",
    "            if (l > k):\n",
    "                l = l-1\n",
    "            \n",
    "            # Define best model from the n = ntries that we trained\n",
    "            i = np.argmax(best_val[k][l])\n",
    "            \n",
    "            # Load the model\n",
    "            if (l < k):\n",
    "                model = load_model(\"CWoLa_signal_m500_models/models_B%d_%d/best_[\" % (counter, run) + str(k) + \",\" + str(l) + \"]_\" + str(i) + \".h5\")\n",
    "            else:\n",
    "                model = load_model(\"CWoLa_signal_m500_models/models_B%d_%d/best_[\" % (counter, run) + str(k) + \",\" + str(l+1) + \"]_\" + str(i) + \".h5\")\n",
    "            \n",
    "            # Rename the model\n",
    "            model.name = \"model\" + str(k) + str(l) + str(i)\n",
    "            \n",
    "            # Save it in the list of models with best validation performance\n",
    "            best_val_models.append(model)\n",
    "            best_val_models_hist.append(models_history[k][l][i])\n",
    "    \n",
    "            # Plot training history\n",
    "            plt.plot(best_val_models_hist[l].history['loss'], label='train')\n",
    "            plt.plot(best_val_models_hist[l].history['val_loss'], label='validation')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('k = %i, l = %i' % (k, l))\n",
    "            plt.legend(loc='upper right')\n",
    "            plt.savefig(\"CWoLa_signal_m500_models/models_B%d_%d/loss_\" % (counter, run) + str(k) + \"_\" + str(l) + \".png\")\n",
    "            plt.show()\n",
    "    \n",
    "        # Build up ensemble model\n",
    "        input_dim = best_val_models[0].layers[0].input.get_shape().as_list()[1]\n",
    "        singleInput = Input((input_dim,))\n",
    "        outs = [model_i(singleInput) for model_i in best_val_models]\n",
    "        outs_merge = Average()(outs)\n",
    "        merged_model = Model(singleInput, outs_merge)\n",
    "    \n",
    "        # Save ensemble models\n",
    "        ensemble_filename = \"CWoLa_signal_m500_models/ensemble_models_B%d_%d/ensemble_\" % (counter, run) + str(k) + \".h5\"\n",
    "        merged_model.save(ensemble_filename)\n",
    "    \n",
    "        # Save preprocessing information for test data\n",
    "        X_test_pre = np.array([X_test_pre_mean, X_test_pre_std])\n",
    "    \n",
    "        # Save test data\n",
    "        X_test_list.append(X_test)\n",
    "        X_test_raw_list.append(X_test_raw)\n",
    "        \n",
    "        # Clear memory\n",
    "        K.clear_session()\n",
    "        gc.collect()\n",
    "        del model\n",
    "        \n",
    "        pass  # End k loop\n",
    "    \n",
    "    \n",
    "    #######################################################################################################\n",
    "    ##########################################   Save to file   ###########################################\n",
    "    #######################################################################################################    \n",
    "    \n",
    "    # Create dataset and classes to store important information\n",
    "    extra = h5py.File('CWoLa_signal_m500_models/extra_info_B%d_%d/extra_info.hdf5' % (counter, run), 'w')\n",
    "\n",
    "    best_val_tofile = extra.create_dataset('best_val', (5, 4, ntries), dtype='f8')\n",
    "\n",
    "    X_test_list_tofile = extra.create_dataset('X_test_list', (5, len(X_test_list[0]), 14), dtype='f8')\n",
    "    X_test_raw_list_tofile = extra.create_dataset('X_test_raw_list', (5, len(X_test_raw_list[0]), 14), dtype='f8')\n",
    "    signal_extra_raw_tofile = extra.create_dataset('signal_extra_raw', (len(signal_extra), 14), dtype='f8')\n",
    "    \n",
    "    bins_tofile = extra.create_dataset('bins', (1, nbin+1), dtype='f8')\n",
    "    X_test_pre_tofile = extra.create_dataset('X_test_pre', (1, 2, 12), dtype='f8')\n",
    "    bench_info_tofile = extra.create_dataset('bench_info', (4, ), dtype='f8')\n",
    "    other_tofile = extra.create_dataset('other', (1, 6), dtype='f8')\n",
    "\n",
    "    # Fill dataset\n",
    "    for k in range(kfolds):\n",
    "        best_val_tofile[k] = best_val[k]\n",
    "        X_test_list_tofile[k] = X_test_list[k]\n",
    "        X_test_raw_list_tofile[k] = X_test_raw_list[k]\n",
    "    \n",
    "    signal_extra_raw_tofile[:] = signal_extra\n",
    "    \n",
    "    bins_tofile[0] = bins\n",
    "    X_test_pre_tofile[0] = X_test_pre\n",
    "    bench_info_tofile[:] = np.array([n_signal, n_background, S_B, S_sqrt_B])\n",
    "    other_tofile[0] = np.array([kfolds, nbin, SR_lower_edge, SR_upper_edge, SB_lower_edge, SB_upper_edge])\n",
    "    \n",
    "    extra.close()\n",
    "    \n",
    "    #######################################################################################################\n",
    "    #########################################   Load from file   ##########################################\n",
    "    #######################################################################################################    \n",
    "    \n",
    "    # Load extra info from file\n",
    "    extra_filename = 'CWoLa_signal_m500_models/extra_info_B%d_%d/extra_info.hdf5' % (counter, run)\n",
    "    h5_results = h5py.File(extra_filename, 'r')\n",
    "\n",
    "    # Store extra info\n",
    "    best_val = h5_results['best_val'][:]\n",
    "    X_test_list = h5_results['X_test_list'][:]\n",
    "    X_test_raw_list = h5_results['X_test_raw_list'][:]\n",
    "    signal_extra_raw = h5_results['signal_extra_raw'][:]\n",
    "    bins = h5_results['bins'][0,:]\n",
    "    X_test_pre = h5_results['X_test_pre'][0,:]\n",
    "    n_signal = int(h5_results['bench_info'][0])\n",
    "    n_background = int(h5_results['bench_info'][1])\n",
    "    S_B = h5_results['bench_info'][2]\n",
    "    S_sqrt_B = h5_results['bench_info'][3]\n",
    "    kfolds = int(h5_results['other'][0,0])\n",
    "    nbin = int(h5_results['other'][0,1])\n",
    "    SR_lower_edge = int(h5_results['other'][0,2])\n",
    "    SR_upper_edge = int(h5_results['other'][0,3])\n",
    "    SB_lower_edge = int(h5_results['other'][0,4])\n",
    "    SB_upper_edge = int(h5_results['other'][0,5])\n",
    "    \n",
    "    h5_results.close()\n",
    "\n",
    "    \n",
    "    #######################################################################################################\n",
    "    #############################################   Testing   #############################################\n",
    "    #######################################################################################################      \n",
    "    \n",
    "    # Preprocess extra signal data\n",
    "    X_test_pre_mean = X_test_pre[0]\n",
    "    X_test_pre_std = X_test_pre[1]\n",
    "    signal_extra[:,:12] = (signal_extra_raw[:,:12] - X_test_pre_mean) / X_test_pre_std\n",
    "    \n",
    "    # Put extra signal events in kfold list\n",
    "    sig_size = int(n_signal_extra/kfolds)\n",
    "    signal_extra_list = []\n",
    "    signal_extra_list_raw = []\n",
    "    \n",
    "    for q in range(kfolds):\n",
    "        signal_extra_list.append(signal_extra[q*sig_size:(q+1)*sig_size])\n",
    "        signal_extra_list_raw.append(signal_extra_raw[q*sig_size:(q+1)*sig_size])\n",
    "        pass\n",
    "    \n",
    "    # Test data\n",
    "    X_ROC = []\n",
    "    Y_ROC = []\n",
    "    all_scores = []\n",
    "\n",
    "    for k in range(kfolds):  # Loop over test\n",
    "    \n",
    "        # Load ensemble model\n",
    "        ensemble_model = load_model(\"CWoLa_signal_m500_models/ensemble_models_B%d_%d/ensemble_\" % (counter, run) + str(k) + \".h5\")\n",
    "    \n",
    "        # Load test k-fold\n",
    "        X_test = X_test_list[k]\n",
    "        X_test_raw = X_test_raw_list[k]\n",
    "        Y_test = X_test[:,13]\n",
    "        \n",
    "        # Load extra signal test k-fold\n",
    "        X_sig_test = signal_extra_list[k]\n",
    "        X_sig_test_raw = signal_extra_list_raw[k]\n",
    "        Y_sig_test = X_sig_test[:,13]\n",
    "        \n",
    "        # Combine both\n",
    "        X_test = np.concatenate((X_test, X_sig_test), axis=0)\n",
    "        X_test_raw = np.concatenate((X_test_raw, X_sig_test_raw), axis=0)\n",
    "        Y_test = np.concatenate((Y_test, Y_sig_test), axis=0)\n",
    "        \n",
    "        X_test, X_test_raw, Y_test = shuffle_3D(X_test, X_test_raw, Y_test)\n",
    "    \n",
    "        # Save data and truth labels for ROC curve\n",
    "        X_ROC.append(X_test)\n",
    "        Y_ROC.append(Y_test)\n",
    "    \n",
    "        # Make predictions with ensemble model on test kfold\n",
    "        print(\"Testing %i...\" % k)\n",
    "        scores_test = ensemble_model.predict(X_test[:,:12], batch_size = nbatch).flatten()\n",
    "    \n",
    "        # Save scores\n",
    "        all_scores.append(scores_test)\n",
    "\n",
    "    # Transform some lists to arrays\n",
    "    X_ROC = np.array(X_ROC).reshape(len(X_ROC)*len(X_ROC[0]), 14)\n",
    "    Y_ROC = np.array(Y_ROC).reshape(len(Y_ROC)*len(Y_ROC[0]), 1)\n",
    "    all_scores = np.array(all_scores).reshape(len(all_scores)*len(all_scores[0]))\n",
    "\n",
    "    # Calculate ROC curve\n",
    "    ROC_scores = all_scores\n",
    "    fpr, tpr, thresholds = roc_curve(Y_ROC, ROC_scores)\n",
    "    \n",
    "    # Save auc\n",
    "    auc_number = auc(fpr, tpr)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.plot(tpr, 1-fpr, label=\"CWoLa (AUC = %.2f)\" % auc_number)\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.title('S/B = %.4f' % S_B)\n",
    "    plt.savefig(\"CWoLa_signal_m500_models/models_B%d_%d/ROC_curve\" % (counter, run) + \".png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot scores for signal and background separately\n",
    "    scores_mean = np.mean(all_scores)\n",
    "    n1 = len(all_scores[all_scores < scores_mean])/len(all_scores)\n",
    "    n2 = len(all_scores[all_scores > scores_mean])/len(all_scores)\n",
    "    plt.hist(all_scores, bins=60, range=(0.4, 0.6), alpha=0.5, color='r', label='N < %.3f = %.2f \\nN > %.3f = %.2f \\nsig_eff_train = %.3f \\nsig_eff_val = %.3f ' % (scores_mean, n1, scores_mean, n2, sig_eff_train_mean, sig_eff_val_mean))\n",
    "    plt.xlabel(r'scores')\n",
    "    plt.ylabel('Events')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Score distribution')\n",
    "    plt.savefig(\"CWoLa_signal_m500_models/models_B%d_%d/score_distribution\" % (counter, run) + \".png\")\n",
    "    plt.show()\n",
    "\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    del(ensemble_model)\n",
    "    \n",
    "    #######################################################################################################\n",
    "    ##########################################   Save to file   ###########################################\n",
    "    #######################################################################################################    \n",
    "    \n",
    "    # Create dataset and classes to store important information\n",
    "    extra = h5py.File('CWoLa_signal_m500_models/extra_info_B%d_%d/test_results.hdf5' % (counter, run), 'w')\n",
    "    \n",
    "    info_tofile = extra.create_dataset('info', (1, ), dtype='f8')\n",
    "    info_tofile[:] = auc_number\n",
    "    \n",
    "    extra.close()\n",
    "    \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
